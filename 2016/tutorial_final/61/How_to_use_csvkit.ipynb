{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use csvkit\n",
    "### What is csvkit?\n",
    "csvkit is a suite of command-line tools for converting to and working with CSV.\n",
    "### Why csvkit?\n",
    "For many different types of data processing operations, all we need is a single line command using csvkit.\n",
    "## 1. Getting started\n",
    "### 1.1. Installing csvkit\n",
    "csvkit can be easily installed using pip:  \n",
    "\n",
    ">$ pip install csvkit\n",
    "\n",
    "### 1.2. Fetching the data\n",
    "This tutorial will introduce how to use csvkit by analyzing real datasets: One dataset [Fish Springs National Wildlife Refuge Odonata records 2015](https://catalog.data.gov/dataset/fish-springs-national-wildlife-refuge-odonata-records-2015) is in Excel format, another dataset [Most Popular Baby Girl Names, 1980-2013](https://catalog.data.gov/dataset/most-popular-baby-girl-names-1980-2013-3f00a) is in CSV format. Use curl to fetch and rename them to odonata.xlsx and name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 11905  100 11905    0     0   8208      0  0:00:01  0:00:01 --:--:--  8564\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16322    0 16322    0     0  49328      0 --:--:-- --:--:-- --:--:-- 74871\n"
     ]
    }
   ],
   "source": [
    "!curl -o odonata.xlsx https://ecos.fws.gov/ServCat/DownloadFile/102067?Reference=61548\n",
    "!curl -o name.csv https://data.illinois.gov/api/views/rx64-7qfr/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. A core feature: pipeability\n",
    "All the csvkit tools take input from standard input and write output to standard output, which means the output of one csvkit tool can become the input of another. Combined with the standard command-line pipe, csvkit tools can form a data processing pipeline without creating unnecessary intermediary files. Keep this feature in mind and we will demonstrate the power of \"csvkit plus pipe\" later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic usage\n",
    "\n",
    "csvkit tools can be categorized into three main types: input, output, and processing.\n",
    "\n",
    "### 2.1. Input\n",
    "### in2csv\n",
    "in2csv converts various tabular data formats (csv, fixed, geojson, json, ndjson, xls, xlsx) into CSV format.\n",
    "\n",
    "It's convenient to get usage information about a csvkit tool with \"-h\" argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: in2csv [-h] [-d DELIMITER] [-t] [-q QUOTECHAR] [-u {0,1,2,3}] [-b]\r\n",
      "              [-p ESCAPECHAR] [-z MAXFIELDSIZE] [-e ENCODING] [-S] [-H] [-v]\r\n",
      "              [-l] [--zero] [-f FILETYPE] [-s SCHEMA] [-k KEY] [-y SNIFFLIMIT]\r\n",
      "              [--sheet SHEET] [--no-inference]\r\n",
      "              [FILE]\r\n",
      "\r\n",
      "Convert common, but less awesome, tabular data formats to CSV.\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  FILE                  The CSV file to operate on. If omitted, will accept\r\n",
      "                        input on STDIN.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -d DELIMITER, --delimiter DELIMITER\r\n",
      "                        Delimiting character of the input CSV file.\r\n",
      "  -t, --tabs            Specifies that the input CSV file is delimited with\r\n",
      "                        tabs. Overrides \"-d\".\r\n",
      "  -q QUOTECHAR, --quotechar QUOTECHAR\r\n",
      "                        Character used to quote strings in the input CSV file.\r\n",
      "  -u {0,1,2,3}, --quoting {0,1,2,3}\r\n",
      "                        Quoting style used in the input CSV file. 0 = Quote\r\n",
      "                        Minimal, 1 = Quote All, 2 = Quote Non-numeric, 3 =\r\n",
      "                        Quote None.\r\n",
      "  -b, --doublequote     Whether or not double quotes are doubled in the input\r\n",
      "                        CSV file.\r\n",
      "  -p ESCAPECHAR, --escapechar ESCAPECHAR\r\n",
      "                        Character used to escape the delimiter if --quoting 3\r\n",
      "                        (\"Quote None\") is specified and to escape the\r\n",
      "                        QUOTECHAR if --doublequote is not specified.\r\n",
      "  -z MAXFIELDSIZE, --maxfieldsize MAXFIELDSIZE\r\n",
      "                        Maximum length of a single field in the input CSV\r\n",
      "                        file.\r\n",
      "  -e ENCODING, --encoding ENCODING\r\n",
      "                        Specify the encoding the input CSV file.\r\n",
      "  -S, --skipinitialspace\r\n",
      "                        Ignore whitespace immediately following the delimiter.\r\n",
      "  -H, --no-header-row   Specifies that the input CSV file has no header row.\r\n",
      "                        Will create default headers.\r\n",
      "  -v, --verbose         Print detailed tracebacks when errors occur.\r\n",
      "  -l, --linenumbers     Insert a column of line numbers at the front of the\r\n",
      "                        output. Useful when piping to grep or as a simple\r\n",
      "                        primary key.\r\n",
      "  --zero                When interpreting or displaying column numbers, use\r\n",
      "                        zero-based numbering instead of the default 1-based\r\n",
      "                        numbering.\r\n",
      "  -f FILETYPE, --format FILETYPE\r\n",
      "                        The format of the input file. If not specified will be\r\n",
      "                        inferred from the file type. Supported formats: csv,\r\n",
      "                        dbf, fixed, geojson, json, ndjson, xls, xlsx.\r\n",
      "  -s SCHEMA, --schema SCHEMA\r\n",
      "                        Specifies a CSV-formatted schema file for converting\r\n",
      "                        fixed-width files. See documentation for details.\r\n",
      "  -k KEY, --key KEY     Specifies a top-level key to use look within for a\r\n",
      "                        list of objects to be converted when processing JSON.\r\n",
      "  -y SNIFFLIMIT, --snifflimit SNIFFLIMIT\r\n",
      "                        Limit CSV dialect sniffing to the specified number of\r\n",
      "                        bytes. Specify \"0\" to disable sniffing entirely.\r\n",
      "  --sheet SHEET         The name of the XLSX sheet to operate on.\r\n",
      "  --no-inference        Disable type inference when parsing the input.\r\n",
      "\r\n",
      "Some command line flags only pertain to specific input formats.\r\n"
     ]
    }
   ],
   "source": [
    "!in2csv -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert odonata.xlsx from Excel into CSV format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Name,Family,Scientific Name,State,County,Location,Lat.,Long.,Elev.,Date,Collector,#,Repository,Determination\r\n",
      "Common Green Darner,Aeshnidae,Anax junius,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",2,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Paiute Dancer,Coenagrionidae,Argia alberta,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",11,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Black-fronted Forktail,Coenagrionidae,Ischnura denticollis,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",15,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Western Pondhawk,Libellulidae,Erythemis collocata,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",3,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Comanche Skimmer,Libellulidae,Libellula comanche,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",6,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Desert Whitetail,Libellulidae,Plathemis subornata,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Variegated Meadowhawk,Libellulidae,Sympetrum corruptum,Utah,Juab,Baker Hot Springs (7 miles north of Brush Wellman Road),39.61039,-112.73041,4628 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Flame Skimmer,Libellulidae,Libellula saturata,Utah,Juab,Fish Springs National Wildlife Refuge; Middle Spring,39.84147,-113.39454,4325 ft,2015-08-13,\"Bybee, Clark & Myrup\",4,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Black-fronted Forktail,Coenagrionidae,Ischnura denticollis,Utah,Juab,Fish Springs National Wildlife Refuge; Middle Spring,39.84147,-113.39454,4325 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Band-winged Meadowhawk,Libellulidae,Sympetrum semicinctum,Utah,Juab,Fish Springs National Wildlife Refuge; Middle Spring,39.84147,-113.39454,4325 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Giant Darner,Aeshnidae,Anax walsinghami,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",2,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Blue-eyed Darner,Aeshnidae,Rhionaeschna multicolor,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",5,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Band-winged Meadowhawk,Libellulidae,Sympetrum semicinctum,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",4,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Flame Skimmer,Libellulidae,Libellula saturata,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Bleached Skimmer,Libellulidae,Libellula composita,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",2,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Paiute Dancer,Coenagrionidae,Argia alberta,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Familiar Bluet,Coenagrionidae,Enallagma civile,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",9,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Tule Bluet,Coenagrionidae,Enallagma carunculatum,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",3,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Black-fronted Forktail,Coenagrionidae,Ischnura denticollis,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",3,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Black Saddlebags,Libellulidae,Tramea lacerata,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "Desert Whitetail,Libellulidae,Plathemis subornata,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",1,BYUC (DNA Tissue),Myrup 2015\r\n",
      "White-belted Ringtail,Gomphidae,Erpetogomphus compositus,Utah,Juab,Fish Springs National Wildlife Refuge; North Spring,39.88718,-113.41305,4310 ft,2015-08-13,\"Bybee, Clark & Myrup\",3,BYUC (DNA Tissue),Myrup 2015\r\n",
      ",,,,,,,,,,,,,\r\n",
      ",,,,,,,,,,,,,\r\n",
      ",,,,,,,,,,,,,\r\n",
      ",,,,,,,,,,,,,\r\n",
      ",,,,,,,,,,,,,\r\n"
     ]
    }
   ],
   "source": [
    "!in2csv odonata.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sending the output to the terminal, it's more useful to write the output to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!in2csv odonata.xlsx > odonata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run in2csv for a CSV file, the formatting (quoting, line endings, etc.) of the file will be standardized. However, it will be too long to display all the output. Remember we can combine pipe with csvkit? Let's use the head command to display the first sixty lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank,Year,Name,Frequency\r\n",
      "1,1980,Jennifer,3017\r\n",
      "2,1980,Amanda,1750\r\n",
      "3,1980,Melissa,1566\r\n",
      "4,1980,Sarah,1390\r\n",
      "5,1980,Jessica,1373\r\n",
      "6,1980,Nicole,1336\r\n",
      "7,1980,Elizabeth,1221\r\n",
      "8,1980,Michelle,1170\r\n",
      "9,1980,Amy,1013\r\n",
      "10,1980,Tiffany,941\r\n",
      "11,1980,Angela,924\r\n",
      "12,1980,Kelly,895\r\n",
      "13,1980,Heather,893\r\n",
      "14,1980,Kimberly,854\r\n",
      "15,1980,Lisa,845\r\n",
      "16,1980,Stephanie,839\r\n",
      "17,1980,Christina,778\r\n",
      "18,1980,Rebecca,757\r\n",
      "19,1980,Laura,756\r\n",
      "20,1980,Erin,736\r\n",
      "21,1980,Mary,655\r\n",
      "22,1980,Jamie,630\r\n",
      "23,1980,Megan,604\r\n",
      "24,1980,Rachel,603\r\n",
      "25,1980,Sara,601\r\n",
      "1,1981,Jennifer,2920\r\n",
      "2,1981,Jessica,1683\r\n",
      "3,1981,Amanda,1607\r\n",
      "4,1981,Sarah,1565\r\n",
      "5,1981,Melissa,1401\r\n",
      "6,1981,Nicole,1341\r\n",
      "7,1981,Elizabeth,1221\r\n",
      "8,1981,Michelle,1024\r\n",
      "9,1981,Amy,1015\r\n",
      "10,1981,Stephanie,976\r\n",
      "11,1981,Tiffany,921\r\n",
      "12,1981,Rebecca,893\r\n",
      "13,1981,Kimberly,805\r\n",
      "14,1981,Angel,804\r\n",
      "15,1981,Lisa,778\r\n",
      "16,1981,Erin,771\r\n",
      "17,1981,Heather,770\r\n",
      "18,1981,Laura,742\r\n",
      "19,1981,Kelly,708\r\n",
      "20,1981,Christina,636\r\n",
      "21,1981,Sara,625\r\n",
      "22,1981,Mary,624\r\n",
      "23,1981,Kristin,599\r\n",
      "24,1981,Rachel,596\r\n",
      "25,1981,Jamie,590\r\n",
      "1,1982,Jennifer,2809\r\n",
      "2,1982,Jessica,1869\r\n",
      "3,1982,Amanda,1547\r\n",
      "4,1982,Nicole,1517\r\n",
      "5,1982,Sarah,1443\r\n",
      "6,1982,Elizabeth,1241\r\n",
      "7,1982,Melissa,1219\r\n",
      "8,1982,Stephanie,1046\r\n",
      "9,1982,Michelle,1023\r\n"
     ]
    }
   ],
   "source": [
    "!in2csv name.csv | head -n60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Output\n",
    "### csvlook\n",
    "csvlook renders a CSV file to the command line in a readable format.  \n",
    "\n",
    "Let's take a look at name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------------|\r\n",
      "|  Rank | Year | Name      | Frequency  |\r\n",
      "|-------+------+-----------+------------|\r\n",
      "|  1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  2    | 1980 | Amanda    | 1750       |\r\n",
      "|  3    | 1980 | Melissa   | 1566       |\r\n",
      "|  4    | 1980 | Sarah     | 1390       |\r\n",
      "|  5    | 1980 | Jessica   | 1373       |\r\n",
      "|  6    | 1980 | Nicole    | 1336       |\r\n",
      "|  7    | 1980 | Elizabeth | 1221       |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook name.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if a table is too wide, the output of csvlook may not be easy to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------------------------+----------------+--------------------------+-------+--------+---------------------------------------------------------+----------+------------+---------+------------+----------------------+----+-------------------+----------------|\r\n",
      "|  Common Name            | Family         | Scientific Name          | State | County | Location                                                | Lat.     | Long.      | Elev.   | Date       | Collector            | #  | Repository        | Determination  |\r\n",
      "|-------------------------+----------------+--------------------------+-------+--------+---------------------------------------------------------+----------+------------+---------+------------+----------------------+----+-------------------+----------------|\r\n",
      "|  Common Green Darner    | Aeshnidae      | Anax junius              | Utah  | Juab   | Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft | 2015-08-13 | Bybee, Clark & Myrup | 2  | BYUC (DNA Tissue) | Myrup 2015     |\r\n",
      "|  Paiute Dancer          | Coenagrionidae | Argia alberta            | Utah  | Juab   | Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft | 2015-08-13 | Bybee, Clark & Myrup | 11 | BYUC (DNA Tissue) | Myrup 2015     |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | Utah  | Juab   | Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft | 2015-08-13 | Bybee, Clark & Myrup | 15 | BYUC (DNA Tissue) | Myrup 2015     |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook odonata.csv | head -n6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tool to truncate the table would be helpful which will be introduced later.\n",
    "\n",
    "### csvformat\n",
    "csvformat converts a CSV file to a custom output format.  \n",
    "\n",
    "We can convert name.csv from a comma-delimited to a pipe-delimited one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank|Year|Name|Frequency\r\n",
      "1|1980|Jennifer|3017\r\n",
      "2|1980|Amanda|1750\r\n",
      "3|1980|Melissa|1566\r\n",
      "4|1980|Sarah|1390\r\n",
      "5|1980|Jessica|1373\r\n",
      "6|1980|Nicole|1336\r\n",
      "7|1980|Elizabeth|1221\r\n",
      "8|1980|Michelle|1170\r\n",
      "9|1980|Amy|1013\r\n"
     ]
    }
   ],
   "source": [
    "!csvformat -D \"|\" name.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to take the output of csvformat as the input of in2csv to see the result of standardizing the formatting of the CSV file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: in2csv [-h] [-d DELIMITER] [-t] [-q QUOTECHAR] [-u {0,1,2,3}] [-b]\r\n",
      "              [-p ESCAPECHAR] [-z MAXFIELDSIZE] [-e ENCODING] [-S] [-H] [-v]\r\n",
      "              [-l] [--zero] [-f FILETYPE] [-s SCHEMA] [-k KEY] [-y SNIFFLIMIT]\r\n",
      "              [--sheet SHEET] [--no-inference]\r\n",
      "              [FILE]\r\n",
      "in2csv: error: You must specify a format when providing data via STDIN (pipe).\r\n"
     ]
    }
   ],
   "source": [
    "!csvformat -D \"|\" name.csv | head | in2csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops... What's wrong? The error message told us how to fix it. For in2csv, we have to specify a format when providing data via standard input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank,Year,Name,Frequency\r\n",
      "1,1980,Jennifer,3017\r\n",
      "2,1980,Amanda,1750\r\n",
      "3,1980,Melissa,1566\r\n",
      "4,1980,Sarah,1390\r\n",
      "5,1980,Jessica,1373\r\n",
      "6,1980,Nicole,1336\r\n",
      "7,1980,Elizabeth,1221\r\n",
      "8,1980,Michelle,1170\r\n",
      "9,1980,Amy,1013\r\n"
     ]
    }
   ],
   "source": [
    "!csvformat -D \"|\" name.csv | head | in2csv -f csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvstat\n",
    "csvstat intelligently determines the type of each column and performs basic statistics for all columns.\n",
    "\n",
    "Let's examine the summary statistics for name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Rank\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 1\r\n",
      "\tMax: 25\r\n",
      "\tSum: 11075\r\n",
      "\tMean: 13.0141010576\r\n",
      "\tMedian: 13\r\n",
      "\tStandard Deviation: 7.21858083478\r\n",
      "\tUnique values: 25\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t25:\t35\r\n",
      "\t\t24:\t34\r\n",
      "\t\t20:\t34\r\n",
      "\t\t21:\t34\r\n",
      "\t\t22:\t34\r\n",
      "  2. Year\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 1980\r\n",
      "\tMax: 2013\r\n",
      "\tSum: 1699011\r\n",
      "\tMean: 1996.48766157\r\n",
      "\tMedian: 1996\r\n",
      "\tStandard Deviation: 9.81153907382\r\n",
      "\tUnique values: 34\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t1986:\t26\r\n",
      "\t\t1987:\t25\r\n",
      "\t\t1984:\t25\r\n",
      "\t\t1985:\t25\r\n",
      "\t\t1982:\t25\r\n",
      "  3. Name\r\n",
      "\t<type 'unicode'>\r\n",
      "\tNulls: False\r\n",
      "\tUnique values: 102\r\n",
      "\t5 most frequent values:\r\n",
      "\t\tSarah:\t26\r\n",
      "\t\tJessica:\t26\r\n",
      "\t\tElizabeth:\t26\r\n",
      "\t\tJennifer:\t24\r\n",
      "\t\tLauren:\t24\r\n",
      "\tMax length: 9\r\n",
      "  4. Frequency\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 264\r\n",
      "\tMax: 3017\r\n",
      "\tSum: 646676\r\n",
      "\tMean: 759.901292597\r\n",
      "\tMedian: 664\r\n",
      "\tStandard Deviation: 387.063122755\r\n",
      "\tUnique values: 586\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t592:\t5\r\n",
      "\t\t630:\t5\r\n",
      "\t\t600:\t5\r\n",
      "\t\t594:\t4\r\n",
      "\t\t596:\t4\r\n",
      "\r\n",
      "Row count: 851\r\n"
     ]
    }
   ],
   "source": [
    "!csvstat name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the columns we care about and only get the statistics of these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Rank\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 1\r\n",
      "\tMax: 25\r\n",
      "\tSum: 11075\r\n",
      "\tMean: 13.0141010576\r\n",
      "\tMedian: 13\r\n",
      "\tStandard Deviation: 7.21858083478\r\n",
      "\tUnique values: 25\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t25:\t35\r\n",
      "\t\t24:\t34\r\n",
      "\t\t20:\t34\r\n",
      "\t\t21:\t34\r\n",
      "\t\t22:\t34\r\n",
      "  2. Year\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 1980\r\n",
      "\tMax: 2013\r\n",
      "\tSum: 1699011\r\n",
      "\tMean: 1996.48766157\r\n",
      "\tMedian: 1996\r\n",
      "\tStandard Deviation: 9.81153907382\r\n",
      "\tUnique values: 34\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t1986:\t26\r\n",
      "\t\t1987:\t25\r\n",
      "\t\t1984:\t25\r\n",
      "\t\t1985:\t25\r\n",
      "\t\t1982:\t25\r\n",
      "\r\n",
      "Row count: 851\r\n"
     ]
    }
   ],
   "source": [
    "!csvstat -c 1,2 name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also replace the column numbers with column names and get the exactly same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Rank\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 1\r\n",
      "\tMax: 25\r\n",
      "\tSum: 11075\r\n",
      "\tMean: 13.0141010576\r\n",
      "\tMedian: 13\r\n",
      "\tStandard Deviation: 7.21858083478\r\n",
      "\tUnique values: 25\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t25:\t35\r\n",
      "\t\t24:\t34\r\n",
      "\t\t20:\t34\r\n",
      "\t\t21:\t34\r\n",
      "\t\t22:\t34\r\n",
      "  2. Year\r\n",
      "\t<type 'int'>\r\n",
      "\tNulls: False\r\n",
      "\tMin: 1980\r\n",
      "\tMax: 2013\r\n",
      "\tSum: 1699011\r\n",
      "\tMean: 1996.48766157\r\n",
      "\tMedian: 1996\r\n",
      "\tStandard Deviation: 9.81153907382\r\n",
      "\tUnique values: 34\r\n",
      "\t5 most frequent values:\r\n",
      "\t\t1986:\t26\r\n",
      "\t\t1987:\t25\r\n",
      "\t\t1984:\t25\r\n",
      "\t\t1985:\t25\r\n",
      "\t\t1982:\t25\r\n",
      "\r\n",
      "Row count: 851\r\n"
     ]
    }
   ],
   "source": [
    "!csvstat -c Rank,Year name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only need one certain kind of statistics, we can specify the statistic argument. For example, we can get the number of unique values of each column by passing the statistic name \"unique\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Rank: 25\r\n",
      "  2. Year: 34\r\n",
      "  3. Name: 102\r\n",
      "  4. Frequency: 586\r\n"
     ]
    }
   ],
   "source": [
    "!csvstat --unique name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can learn that the data in name.csv contains the top 25 popular baby girl names in each of the 34 years. Since there are only 102 different names, we can also learn that many popular names must repeatedly appear in several years.\n",
    "\n",
    "Maybe the unique statistics for frequency doesn't make much sense for now, so let's combine the two filters together to just get the unique information about the first three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Rank: 25\r\n",
      "  2. Year: 34\r\n",
      "  3. Name: 102\r\n"
     ]
    }
   ],
   "source": [
    "!csvstat -c 1,2,3 --unique name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Processing\n",
    "### csvclean\n",
    "csvclean cleans a CSV file of common syntax errors.  \n",
    "\n",
    "Why not use csvclean to test name.csv and odonata.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors.\n",
      "No errors.\n"
     ]
    }
   ],
   "source": [
    "!csvclean -n name.csv\n",
    "!csvclean -n odonata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a CSV file with syntax errors. Download bad.csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    94  100    94    0     0    168      0 --:--:-- --:--:-- --:--:--   723\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/wireservice/csvkit/0.9.1/examples/bad.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----------+-----------------------+----------+----------------|\r\n",
      "|  column_a | column_b              | column_c  |\r\n",
      "|-----------+-----------------------+----------+----------------|\r\n",
      "|  1        | 27                    |          | I'm too long!  |\r\n",
      "|           | I'm too short!         |\r\n",
      "|  0        | mixed types.... uh oh | 17        |\r\n",
      "|-----------+-----------------------+----------+----------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook bad.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what csvclean will tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1: Expected 3 columns, found 4 columns\r\n",
      "Line 2: Expected 3 columns, found 2 columns\r\n"
     ]
    }
   ],
   "source": [
    "!csvclean -n bad.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run in2csv for bad.csv to standardize the formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----------+-----------------------+-----------|\r\n",
      "|  column_a | column_b              | column_c  |\r\n",
      "|-----------+-----------------------+-----------|\r\n",
      "|  1        | 27                    |           |\r\n",
      "|           | I'm too short!        |           |\r\n",
      "|  0        | mixed types.... uh oh | 17        |\r\n",
      "|-----------+-----------------------+-----------|\r\n"
     ]
    }
   ],
   "source": [
    "!in2csv bad.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvcut\n",
    "csvcut filters and truncates a CSV file.\n",
    "\n",
    "Let's see all the columns in name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Rank\r\n",
      "  2: Year\r\n",
      "  3: Name\r\n",
      "  4: Frequency\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want to take a look at columns 1, 2, and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+------------|\r\n",
      "|  Rank | Year | Name       |\r\n",
      "|-------+------+------------|\r\n",
      "|  1    | 1980 | Jennifer   |\r\n",
      "|  2    | 1980 | Amanda     |\r\n",
      "|  3    | 1980 | Melissa    |\r\n",
      "|  4    | 1980 | Sarah      |\r\n",
      "|  5    | 1980 | Jessica    |\r\n",
      "|  6    | 1980 | Nicole     |\r\n",
      "|  7    | 1980 | Elizabeth  |\r\n",
      "|  8    | 1980 | Michelle   |\r\n",
      "|  9    | 1980 | Amy        |\r\n",
      "|-------+------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c 1,2,3 name.csv | head | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember how ugly the output of csvlook odonata.csv looks like? csvlook can be much more useful as a final operation when piping through other csvkit tools, especially after csvcut. Let's get all the column names in odonata.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Common Name\r\n",
      "  2: Family\r\n",
      "  3: Scientific Name\r\n",
      "  4: State\r\n",
      "  5: County\r\n",
      "  6: Location\r\n",
      "  7: Lat.\r\n",
      "  8: Long.\r\n",
      "  9: Elev.\r\n",
      " 10: Date\r\n",
      " 11: Collector\r\n",
      " 12: #\r\n",
      " 13: Repository\r\n",
      " 14: Determination\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n odonata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the unique statistics to help us select more interesting columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Common Name: 16\r\n",
      "  2. Family: 4\r\n",
      "  3. Scientific Name: 16\r\n",
      "  4. State: 1\r\n",
      "  5. County: 1\r\n",
      "  6. Location: 3\r\n",
      "  7. Lat.: 3\r\n",
      "  8. Long.: 3\r\n",
      "  9. Elev.: 3\r\n",
      " 10. Date: 1\r\n",
      " 11. Collector: 1\r\n",
      " 12. #: 9\r\n",
      " 13. Repository: 1\r\n",
      " 14. Determination: 1\r\n"
     ]
    }
   ],
   "source": [
    "!csvstat --unique odonata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location, Lat., Long., Elev. all have 3 unique values. Let's see if they are corresponding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------------------------------------------+----------+------------+----------|\r\n",
      "|  Location                                                | Lat.     | Long.      | Elev.    |\r\n",
      "|----------------------------------------------------------+----------+------------+----------|\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Baker Hot Springs (7 miles north of Brush Wellman Road) | 39.61039 | -112.73041 | 4628 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; Middle Spring    | 39.84147 | -113.39454 | 4325 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; Middle Spring    | 39.84147 | -113.39454 | 4325 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; Middle Spring    | 39.84147 | -113.39454 | 4325 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|  Fish Springs National Wildlife Refuge; North Spring     | 39.88718 | -113.41305 | 4310 ft  |\r\n",
      "|                                                          |          |            |          |\r\n",
      "|                                                          |          |            |          |\r\n",
      "|                                                          |          |            |          |\r\n",
      "|                                                          |          |            |          |\r\n",
      "|                                                          |          |            |          |\r\n",
      "|----------------------------------------------------------+----------+------------+----------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c 6,7,8,9 odonata.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! So we can use Lat. and Long. to represent location and elevation. Let's select more interesting columns and make the output more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------------------------+----------------+--------------------------+----------+------------+-----|\r\n",
      "|  Common Name            | Family         | Scientific Name          | Lat.     | Long.      | #   |\r\n",
      "|-------------------------+----------------+--------------------------+----------+------------+-----|\r\n",
      "|  Common Green Darner    | Aeshnidae      | Anax junius              | 39.61039 | -112.73041 | 2   |\r\n",
      "|  Paiute Dancer          | Coenagrionidae | Argia alberta            | 39.61039 | -112.73041 | 11  |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | 39.61039 | -112.73041 | 15  |\r\n",
      "|  Western Pondhawk       | Libellulidae   | Erythemis collocata      | 39.61039 | -112.73041 | 3   |\r\n",
      "|  Comanche Skimmer       | Libellulidae   | Libellula comanche       | 39.61039 | -112.73041 | 6   |\r\n",
      "|  Desert Whitetail       | Libellulidae   | Plathemis subornata      | 39.61039 | -112.73041 | 1   |\r\n",
      "|  Variegated Meadowhawk  | Libellulidae   | Sympetrum corruptum      | 39.61039 | -112.73041 | 1   |\r\n",
      "|  Flame Skimmer          | Libellulidae   | Libellula saturata       | 39.84147 | -113.39454 | 4   |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | 39.84147 | -113.39454 | 1   |\r\n",
      "|  Band-winged Meadowhawk | Libellulidae   | Sympetrum semicinctum    | 39.84147 | -113.39454 | 1   |\r\n",
      "|  Giant Darner           | Aeshnidae      | Anax walsinghami         | 39.88718 | -113.41305 | 2   |\r\n",
      "|  Blue-eyed Darner       | Aeshnidae      | Rhionaeschna multicolor  | 39.88718 | -113.41305 | 5   |\r\n",
      "|  Band-winged Meadowhawk | Libellulidae   | Sympetrum semicinctum    | 39.88718 | -113.41305 | 4   |\r\n",
      "|  Flame Skimmer          | Libellulidae   | Libellula saturata       | 39.88718 | -113.41305 | 1   |\r\n",
      "|  Bleached Skimmer       | Libellulidae   | Libellula composita      | 39.88718 | -113.41305 | 2   |\r\n",
      "|  Paiute Dancer          | Coenagrionidae | Argia alberta            | 39.88718 | -113.41305 | 1   |\r\n",
      "|  Familiar Bluet         | Coenagrionidae | Enallagma civile         | 39.88718 | -113.41305 | 9   |\r\n",
      "|  Tule Bluet             | Coenagrionidae | Enallagma carunculatum   | 39.88718 | -113.41305 | 3   |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | 39.88718 | -113.41305 | 3   |\r\n",
      "|  Black Saddlebags       | Libellulidae   | Tramea lacerata          | 39.88718 | -113.41305 | 1   |\r\n",
      "|  Desert Whitetail       | Libellulidae   | Plathemis subornata      | 39.88718 | -113.41305 | 1   |\r\n",
      "|  White-belted Ringtail  | Gomphidae      | Erpetogomphus compositus | 39.88718 | -113.41305 | 3   |\r\n",
      "|                         |                |                          |          |            |     |\r\n",
      "|                         |                |                          |          |            |     |\r\n",
      "|                         |                |                          |          |            |     |\r\n",
      "|                         |                |                          |          |            |     |\r\n",
      "|                         |                |                          |          |            |     |\r\n",
      "|-------------------------+----------------+--------------------------+----------+------------+-----|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c 1,2,3,7,8,12 odonata.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that there are several empty rows in odonata.csv. Let's strip them by adding -x argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------------------------+----------------+--------------------------+----------+------------+-----|\r\n",
      "|  Common Name            | Family         | Scientific Name          | Lat.     | Long.      | #   |\r\n",
      "|-------------------------+----------------+--------------------------+----------+------------+-----|\r\n",
      "|  Common Green Darner    | Aeshnidae      | Anax junius              | 39.61039 | -112.73041 | 2   |\r\n",
      "|  Paiute Dancer          | Coenagrionidae | Argia alberta            | 39.61039 | -112.73041 | 11  |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | 39.61039 | -112.73041 | 15  |\r\n",
      "|  Western Pondhawk       | Libellulidae   | Erythemis collocata      | 39.61039 | -112.73041 | 3   |\r\n",
      "|  Comanche Skimmer       | Libellulidae   | Libellula comanche       | 39.61039 | -112.73041 | 6   |\r\n",
      "|  Desert Whitetail       | Libellulidae   | Plathemis subornata      | 39.61039 | -112.73041 | 1   |\r\n",
      "|  Variegated Meadowhawk  | Libellulidae   | Sympetrum corruptum      | 39.61039 | -112.73041 | 1   |\r\n",
      "|  Flame Skimmer          | Libellulidae   | Libellula saturata       | 39.84147 | -113.39454 | 4   |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | 39.84147 | -113.39454 | 1   |\r\n",
      "|  Band-winged Meadowhawk | Libellulidae   | Sympetrum semicinctum    | 39.84147 | -113.39454 | 1   |\r\n",
      "|  Giant Darner           | Aeshnidae      | Anax walsinghami         | 39.88718 | -113.41305 | 2   |\r\n",
      "|  Blue-eyed Darner       | Aeshnidae      | Rhionaeschna multicolor  | 39.88718 | -113.41305 | 5   |\r\n",
      "|  Band-winged Meadowhawk | Libellulidae   | Sympetrum semicinctum    | 39.88718 | -113.41305 | 4   |\r\n",
      "|  Flame Skimmer          | Libellulidae   | Libellula saturata       | 39.88718 | -113.41305 | 1   |\r\n",
      "|  Bleached Skimmer       | Libellulidae   | Libellula composita      | 39.88718 | -113.41305 | 2   |\r\n",
      "|  Paiute Dancer          | Coenagrionidae | Argia alberta            | 39.88718 | -113.41305 | 1   |\r\n",
      "|  Familiar Bluet         | Coenagrionidae | Enallagma civile         | 39.88718 | -113.41305 | 9   |\r\n",
      "|  Tule Bluet             | Coenagrionidae | Enallagma carunculatum   | 39.88718 | -113.41305 | 3   |\r\n",
      "|  Black-fronted Forktail | Coenagrionidae | Ischnura denticollis     | 39.88718 | -113.41305 | 3   |\r\n",
      "|  Black Saddlebags       | Libellulidae   | Tramea lacerata          | 39.88718 | -113.41305 | 1   |\r\n",
      "|  Desert Whitetail       | Libellulidae   | Plathemis subornata      | 39.88718 | -113.41305 | 1   |\r\n",
      "|  White-belted Ringtail  | Gomphidae      | Erpetogomphus compositus | 39.88718 | -113.41305 | 3   |\r\n",
      "|-------------------------+----------------+--------------------------+----------+------------+-----|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -x -c 1,2,3,7,8,12 odonata.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvgrep\n",
    "csvgrep filters data to rows where certain columns match a pattern or a value.  \n",
    "\n",
    "Let's search for the row relating to the year of 1980 in name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------------|\r\n",
      "|  Rank | Year | Name      | Frequency  |\r\n",
      "|-------+------+-----------+------------|\r\n",
      "|  1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  2    | 1980 | Amanda    | 1750       |\r\n",
      "|  3    | 1980 | Melissa   | 1566       |\r\n",
      "|  4    | 1980 | Sarah     | 1390       |\r\n",
      "|  5    | 1980 | Jessica   | 1373       |\r\n",
      "|  6    | 1980 | Nicole    | 1336       |\r\n",
      "|  7    | 1980 | Elizabeth | 1221       |\r\n",
      "|  8    | 1980 | Michelle  | 1170       |\r\n",
      "|  9    | 1980 | Amy       | 1013       |\r\n",
      "|  10   | 1980 | Tiffany   | 941        |\r\n",
      "|  11   | 1980 | Angela    | 924        |\r\n",
      "|  12   | 1980 | Kelly     | 895        |\r\n",
      "|  13   | 1980 | Heather   | 893        |\r\n",
      "|  14   | 1980 | Kimberly  | 854        |\r\n",
      "|  15   | 1980 | Lisa      | 845        |\r\n",
      "|  16   | 1980 | Stephanie | 839        |\r\n",
      "|  17   | 1980 | Christina | 778        |\r\n",
      "|  18   | 1980 | Rebecca   | 757        |\r\n",
      "|  19   | 1980 | Laura     | 756        |\r\n",
      "|  20   | 1980 | Erin      | 736        |\r\n",
      "|  21   | 1980 | Mary      | 655        |\r\n",
      "|  22   | 1980 | Jamie     | 630        |\r\n",
      "|  23   | 1980 | Megan     | 604        |\r\n",
      "|  24   | 1980 | Rachel    | 603        |\r\n",
      "|  25   | 1980 | Sara      | 601        |\r\n",
      "|-------+------+-----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvgrep -c 2 -m 1980 name.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the row relating to the name starting with \"B\" in name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+----------+------------|\r\n",
      "|  Rank | Year | Name     | Frequency  |\r\n",
      "|-------+------+----------+------------|\r\n",
      "|  18   | 1985 | Brittany | 714        |\r\n",
      "|  9    | 1986 | Brittany | 952        |\r\n",
      "|  8    | 1987 | Brittany | 1056       |\r\n",
      "|  6    | 1988 | Brittany | 1223       |\r\n",
      "|  4    | 1989 | Brittany | 1562       |\r\n",
      "|  3    | 1990 | Brittany | 1533       |\r\n",
      "|  4    | 1991 | Brittany | 1286       |\r\n",
      "|  6    | 1992 | Brittany | 1065       |\r\n",
      "|  7    | 1993 | Brittany | 953        |\r\n",
      "|  9    | 1994 | Brittany | 825        |\r\n",
      "|  16   | 1995 | Brittany | 670        |\r\n",
      "|  20   | 1995 | Brianna  | 538        |\r\n",
      "|  19   | 1996 | Brittany | 582        |\r\n",
      "|  21   | 1996 | Brianna  | 527        |\r\n",
      "|  20   | 1997 | Brianna  | 534        |\r\n",
      "|  23   | 1997 | Brittany | 476        |\r\n",
      "|  20   | 1998 | Brianna  | 497        |\r\n",
      "|  20   | 1999 | Brianna  | 522        |\r\n",
      "|  19   | 2000 | Brianna  | 533        |\r\n",
      "|  21   | 2001 | Brianna  | 484        |\r\n",
      "|  20   | 2002 | Brianna  | 459        |\r\n",
      "|  18   | 2003 | Brianna  | 471        |\r\n",
      "|  23   | 2004 | Brianna  | 403        |\r\n",
      "|  21   | 2006 | BRIANNA  | 385        |\r\n",
      "|  20   | 2008 | BRIANNA  | 363        |\r\n",
      "|  24   | 2011 | BROOKLYN | 286        |\r\n",
      "|  21   | 2012 | BROOKLYN | 313        |\r\n",
      "|  24   | 2013 | BROOKLYN | 272        |\r\n",
      "|-------+------+----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvgrep -c 3 -r \"^B\" name.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvsort\n",
    "csvsort sorts a CSV file.  \n",
    "\n",
    "We can sort name.csv by the \"Rank\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------------|\r\n",
      "|  Rank | Year | Name      | Frequency  |\r\n",
      "|-------+------+-----------+------------|\r\n",
      "|  1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  1    | 1981 | Jennifer  | 2920       |\r\n",
      "|  1    | 1982 | Jennifer  | 2809       |\r\n",
      "|  1    | 1983 | Jennifer  | 2636       |\r\n",
      "|  1    | 1984 | Jennifer  | 2528       |\r\n",
      "|  1    | 1985 | Jennifer  | 2096       |\r\n",
      "|  1    | 1986 | Jessica   | 2198       |\r\n",
      "|  1    | 1987 | Ashley    | 2565       |\r\n",
      "|  1    | 1988 | Jessica   | 2275       |\r\n",
      "|  1    | 1989 | Ashley    | 2308       |\r\n",
      "|  1    | 1990 | Jessica   | 2117       |\r\n",
      "|  1    | 1991 | Jessica   | 2075       |\r\n",
      "|  1    | 1992 | Jessica   | 1816       |\r\n",
      "|  1    | 1993 | Jessica   | 1789       |\r\n",
      "|  1    | 1994 | Jessica   | 1640       |\r\n",
      "|  1    | 1995 | Jessica   | 1429       |\r\n",
      "|  1    | 1996 | Jessica   | 1265       |\r\n",
      "|  1    | 1997 | Emily     | 1213       |\r\n",
      "|  1    | 1998 | Emily     | 1211       |\r\n",
      "|  1    | 1999 | Emily     | 1210       |\r\n",
      "|  1    | 2000 | Emily     | 1205       |\r\n",
      "|  1    | 2001 | Emily     | 1182       |\r\n",
      "|  1    | 2002 | Emily     | 1117       |\r\n",
      "|  1    | 2003 | Emily     | 1132       |\r\n",
      "|  1    | 2004 | Emily     | 1094       |\r\n",
      "|  1    | 2005 | Emily     | 1015       |\r\n",
      "|  1    | 2006 | EMILY     | 937        |\r\n",
      "|  1    | 2007 | ISABELLA  | 893        |\r\n",
      "|  1    | 2008 | OLIVIA    | 850        |\r\n",
      "|  1    | 2009 | ISABELLA  | 971        |\r\n",
      "|  1    | 2010 | ISABELLA  | 940        |\r\n",
      "|  1    | 2011 | SOPHIA    | 890        |\r\n",
      "|  1    | 2012 | SOPHIA    | 957        |\r\n",
      "|  1    | 2013 | OLIVIA    | 856        |\r\n",
      "|  2    | 1980 | Amanda    | 1750       |\r\n",
      "|  2    | 1981 | Jessica   | 1683       |\r\n",
      "|  2    | 1982 | Jessica   | 1869       |\r\n",
      "|  2    | 1983 | Jessica   | 1847       |\r\n",
      "|  2    | 1984 | Jessica   | 1844       |\r\n",
      "|  2    | 1985 | Ashley    | 2030       |\r\n",
      "|  2    | 1986 | Ashley    | 2103       |\r\n",
      "|  2    | 1987 | Jessica   | 2521       |\r\n",
      "|  2    | 1988 | Ashley    | 2264       |\r\n",
      "|  2    | 1989 | Jessica   | 2031       |\r\n",
      "|  2    | 1990 | Ashley    | 2098       |\r\n",
      "|  2    | 1991 | Ashley    | 1906       |\r\n",
      "|  2    | 1992 | Ashley    | 1744       |\r\n",
      "|  2    | 1993 | Ashley    | 1573       |\r\n",
      "|  2    | 1994 | Ashley    | 1300       |\r\n",
      "|  2    | 1995 | Ashley    | 1171       |\r\n",
      "|  2    | 1996 | Emily     | 1172       |\r\n",
      "|  2    | 1997 | Jessica   | 1122       |\r\n",
      "|  2    | 1998 | Samantha  | 1031       |\r\n",
      "|  2    | 1999 | Hannah    | 958        |\r\n",
      "|  2    | 2000 | Hannah    | 1041       |\r\n",
      "|  2    | 2001 | Hannah    | 882        |\r\n",
      "|  2    | 2002 | Emma      | 838        |\r\n",
      "|  2    | 2003 | Emma      | 1046       |\r\n",
      "|  2    | 2004 | Emma      | 885        |\r\n",
      "|  2    | 2005 | Emma      | 840        |\r\n",
      "|  2    | 2006 | EMMA      | 828        |\r\n",
      "|  2    | 2007 | OLIVIA    | 874        |\r\n",
      "|  2    | 2008 | EMILY     | 794        |\r\n",
      "|  2    | 2009 | OLIVIA    | 862        |\r\n",
      "|  2    | 2010 | SOPHIA    | 915        |\r\n",
      "|  2    | 2011 | OLIVIA    | 849        |\r\n",
      "|  2    | 2012 | OLIVIA    | 819        |\r\n",
      "|  2    | 2013 | SOPHIA    | 819        |\r\n",
      "|  3    | 1980 | Melissa   | 1566       |\r\n",
      "|  3    | 1981 | Amanda    | 1607       |\r\n",
      "|  3    | 1982 | Amanda    | 1547       |\r\n",
      "|  3    | 1983 | Amanda    | 1493       |\r\n",
      "|  3    | 1984 | Ashley    | 1577       |\r\n",
      "|  3    | 1985 | Jessica   | 1951       |\r\n",
      "|  3    | 1986 | Jennifer  | 1793       |\r\n",
      "|  3    | 1987 | Amanda    | 1742       |\r\n",
      "|  3    | 1988 | Amanda    | 1686       |\r\n",
      "|  3    | 1989 | Amanda    | 1610       |\r\n",
      "|  3    | 1990 | Brittany  | 1533       |\r\n",
      "|  3    | 1991 | Samantha  | 1313       |\r\n",
      "|  3    | 1992 | Samantha  | 1189       |\r\n",
      "|  3    | 1993 | Samantha  | 1195       |\r\n",
      "|  3    | 1994 | Emily     | 1157       |\r\n",
      "|  3    | 1995 | Emily     | 1169       |\r\n",
      "|  3    | 1996 | Ashley    | 1041       |\r\n",
      "|  3    | 1997 | Samantha  | 1005       |\r\n",
      "|  3    | 1998 | Jessica   | 978        |\r\n",
      "|  3    | 1999 | Samantha  | 901        |\r\n",
      "|  3    | 2000 | Jessica   | 843        |\r\n",
      "|  3    | 2001 | Jessica   | 793        |\r\n",
      "|  3    | 2002 | Madison   | 792        |\r\n",
      "|  3    | 2003 | Olivia    | 808        |\r\n",
      "|  3    | 2004 | Olivia    | 797        |\r\n",
      "|  3    | 2005 | Olivia    | 797        |\r\n",
      "|  3    | 2006 | ISABELLA  | 828        |\r\n",
      "|  3    | 2007 | EMILY     | 815        |\r\n",
      "|  3    | 2008 | EMMA      | 783        |\r\n"
     ]
    }
   ],
   "source": [
    "!csvsort -c Rank name.csv | csvlook | head -n100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvjoin\n",
    "csvjoin merges CSV files on a specified column or columns.  \n",
    "\n",
    "Let's seperate the name.csv to two different CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+------------|\n",
      "|  Rank | Year | Name       |\n",
      "|-------+------+------------|\n",
      "|  1    | 1980 | Jennifer   |\n",
      "|  2    | 1980 | Amanda     |\n",
      "|  3    | 1980 | Melissa    |\n",
      "|  4    | 1980 | Sarah      |\n",
      "|  5    | 1980 | Jessica    |\n",
      "|  6    | 1980 | Nicole     |\n",
      "|  7    | 1980 | Elizabeth  |\n",
      "|  8    | 1980 | Michelle   |\n",
      "|  9    | 1980 | Amy        |\n",
      "|  10   | 1980 | Tiffany    |\n",
      "|  11   | 1980 | Angela     |\n",
      "|  12   | 1980 | Kelly      |\n",
      "|  13   | 1980 | Heather    |\n",
      "|  14   | 1980 | Kimberly   |\n",
      "|  15   | 1980 | Lisa       |\n",
      "|  16   | 1980 | Stephanie  |\n",
      "|  17   | 1980 | Christina  |\n",
      "|  18   | 1980 | Rebecca    |\n",
      "|  19   | 1980 | Laura      |\n",
      "|  20   | 1980 | Erin       |\n",
      "|  21   | 1980 | Mary       |\n",
      "|  22   | 1980 | Jamie      |\n",
      "|  23   | 1980 | Megan      |\n",
      "|  24   | 1980 | Rachel     |\n",
      "|  25   | 1980 | Sara       |\n",
      "|-------+------+------------|\n",
      "|-------+------------|\n",
      "|  Rank | Frequency  |\n",
      "|-------+------------|\n",
      "|  1    | 3017       |\n",
      "|  2    | 1750       |\n",
      "|  3    | 1566       |\n",
      "|  4    | 1390       |\n",
      "|  5    | 1373       |\n",
      "|  6    | 1336       |\n",
      "|  7    | 1221       |\n",
      "|  8    | 1170       |\n",
      "|  9    | 1013       |\n",
      "|  10   | 941        |\n",
      "|  11   | 924        |\n",
      "|  12   | 895        |\n",
      "|  13   | 893        |\n",
      "|  14   | 854        |\n",
      "|  15   | 845        |\n",
      "|  16   | 839        |\n",
      "|  17   | 778        |\n",
      "|  18   | 757        |\n",
      "|  19   | 756        |\n",
      "|  20   | 736        |\n",
      "|  21   | 655        |\n",
      "|  22   | 630        |\n",
      "|  23   | 604        |\n",
      "|  24   | 603        |\n",
      "|  25   | 601        |\n",
      "|-------+------------|\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c 1,2,3 name.csv | csvgrep -c 2 -m 1980 > name_1.csv\n",
    "!csvcut -c 1,2,4 name.csv | csvgrep -c 2 -m 1980 | csvcut -c 1,3 > name_2.csv\n",
    "!csvlook name_1.csv\n",
    "!csvlook name_2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the default inner join to join name_1.csv and name_2.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------+------------|\r\n",
      "|  Rank | Year | Name      | Rank | Frequency  |\r\n",
      "|-------+------+-----------+------+------------|\r\n",
      "|  1    | 1980 | Jennifer  | 1    | 3017       |\r\n",
      "|  2    | 1980 | Amanda    | 2    | 1750       |\r\n",
      "|  3    | 1980 | Melissa   | 3    | 1566       |\r\n",
      "|  4    | 1980 | Sarah     | 4    | 1390       |\r\n",
      "|  5    | 1980 | Jessica   | 5    | 1373       |\r\n",
      "|  6    | 1980 | Nicole    | 6    | 1336       |\r\n",
      "|  7    | 1980 | Elizabeth | 7    | 1221       |\r\n",
      "|  8    | 1980 | Michelle  | 8    | 1170       |\r\n",
      "|  9    | 1980 | Amy       | 9    | 1013       |\r\n",
      "|  10   | 1980 | Tiffany   | 10   | 941        |\r\n",
      "|  11   | 1980 | Angela    | 11   | 924        |\r\n",
      "|  12   | 1980 | Kelly     | 12   | 895        |\r\n",
      "|  13   | 1980 | Heather   | 13   | 893        |\r\n",
      "|  14   | 1980 | Kimberly  | 14   | 854        |\r\n",
      "|  15   | 1980 | Lisa      | 15   | 845        |\r\n",
      "|  16   | 1980 | Stephanie | 16   | 839        |\r\n",
      "|  17   | 1980 | Christina | 17   | 778        |\r\n",
      "|  18   | 1980 | Rebecca   | 18   | 757        |\r\n",
      "|  19   | 1980 | Laura     | 19   | 756        |\r\n",
      "|  20   | 1980 | Erin      | 20   | 736        |\r\n",
      "|  21   | 1980 | Mary      | 21   | 655        |\r\n",
      "|  22   | 1980 | Jamie     | 22   | 630        |\r\n",
      "|  23   | 1980 | Megan     | 23   | 604        |\r\n",
      "|  24   | 1980 | Rachel    | 24   | 603        |\r\n",
      "|  25   | 1980 | Sara      | 25   | 601        |\r\n",
      "|-------+------+-----------+------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvjoin -c Rank name_1.csv name_2.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove the extra Rank column manually if we don't need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------------|\r\n",
      "|  Rank | Year | Name      | Frequency  |\r\n",
      "|-------+------+-----------+------------|\r\n",
      "|  1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  2    | 1980 | Amanda    | 1750       |\r\n",
      "|  3    | 1980 | Melissa   | 1566       |\r\n",
      "|  4    | 1980 | Sarah     | 1390       |\r\n",
      "|  5    | 1980 | Jessica   | 1373       |\r\n",
      "|  6    | 1980 | Nicole    | 1336       |\r\n",
      "|  7    | 1980 | Elizabeth | 1221       |\r\n",
      "|  8    | 1980 | Michelle  | 1170       |\r\n",
      "|  9    | 1980 | Amy       | 1013       |\r\n",
      "|  10   | 1980 | Tiffany   | 941        |\r\n",
      "|  11   | 1980 | Angela    | 924        |\r\n",
      "|  12   | 1980 | Kelly     | 895        |\r\n",
      "|  13   | 1980 | Heather   | 893        |\r\n",
      "|  14   | 1980 | Kimberly  | 854        |\r\n",
      "|  15   | 1980 | Lisa      | 845        |\r\n",
      "|  16   | 1980 | Stephanie | 839        |\r\n",
      "|  17   | 1980 | Christina | 778        |\r\n",
      "|  18   | 1980 | Rebecca   | 757        |\r\n",
      "|  19   | 1980 | Laura     | 756        |\r\n",
      "|  20   | 1980 | Erin      | 736        |\r\n",
      "|  21   | 1980 | Mary      | 655        |\r\n",
      "|  22   | 1980 | Jamie     | 630        |\r\n",
      "|  23   | 1980 | Megan     | 604        |\r\n",
      "|  24   | 1980 | Rachel    | 603        |\r\n",
      "|  25   | 1980 | Sara      | 601        |\r\n",
      "|-------+------+-----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvjoin -c Rank name_1.csv name_2.csv | csvcut -c 1,2,3,5 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvstack\n",
    "csvstack stacks up the rows from multiple CSV files.  \n",
    "\n",
    "Let's extract two parts from name.csv according to the rank value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------------|\n",
      "|  Rank | Year | Name      | Frequency  |\n",
      "|-------+------+-----------+------------|\n",
      "|  1    | 1980 | Jennifer  | 3017       |\n",
      "|  2    | 1980 | Amanda    | 1750       |\n",
      "|  3    | 1980 | Melissa   | 1566       |\n",
      "|  4    | 1980 | Sarah     | 1390       |\n",
      "|  5    | 1980 | Jessica   | 1373       |\n",
      "|  6    | 1980 | Nicole    | 1336       |\n",
      "|  7    | 1980 | Elizabeth | 1221       |\n",
      "|  8    | 1980 | Michelle  | 1170       |\n",
      "|  9    | 1980 | Amy       | 1013       |\n",
      "|-------+------+-----------+------------|\n",
      "|-------+------+-----------+------------|\n",
      "|  Rank | Year | Name      | Frequency  |\n",
      "|-------+------+-----------+------------|\n",
      "|  10   | 1980 | Tiffany   | 941        |\n",
      "|  11   | 1980 | Angela    | 924        |\n",
      "|  12   | 1980 | Kelly     | 895        |\n",
      "|  13   | 1980 | Heather   | 893        |\n",
      "|  14   | 1980 | Kimberly  | 854        |\n",
      "|  15   | 1980 | Lisa      | 845        |\n",
      "|  16   | 1980 | Stephanie | 839        |\n",
      "|  17   | 1980 | Christina | 778        |\n",
      "|  18   | 1980 | Rebecca   | 757        |\n",
      "|  19   | 1980 | Laura     | 756        |\n",
      "|-------+------+-----------+------------|\n"
     ]
    }
   ],
   "source": [
    "!csvgrep -c 2 -m 1980 name.csv | csvgrep -c 1 -r \"^\\d$\" > name_a.csv\n",
    "!csvgrep -c 2 -m 1980 name.csv | csvgrep -c 1 -r \"^1\\d$\" > name_b.csv\n",
    "!csvlook name_a.csv\n",
    "!csvlook name_b.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stack these two CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------+------------|\r\n",
      "|  Rank | Year | Name      | Frequency  |\r\n",
      "|-------+------+-----------+------------|\r\n",
      "|  1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  2    | 1980 | Amanda    | 1750       |\r\n",
      "|  3    | 1980 | Melissa   | 1566       |\r\n",
      "|  4    | 1980 | Sarah     | 1390       |\r\n",
      "|  5    | 1980 | Jessica   | 1373       |\r\n",
      "|  6    | 1980 | Nicole    | 1336       |\r\n",
      "|  7    | 1980 | Elizabeth | 1221       |\r\n",
      "|  8    | 1980 | Michelle  | 1170       |\r\n",
      "|  9    | 1980 | Amy       | 1013       |\r\n",
      "|  10   | 1980 | Tiffany   | 941        |\r\n",
      "|  11   | 1980 | Angela    | 924        |\r\n",
      "|  12   | 1980 | Kelly     | 895        |\r\n",
      "|  13   | 1980 | Heather   | 893        |\r\n",
      "|  14   | 1980 | Kimberly  | 854        |\r\n",
      "|  15   | 1980 | Lisa      | 845        |\r\n",
      "|  16   | 1980 | Stephanie | 839        |\r\n",
      "|  17   | 1980 | Christina | 778        |\r\n",
      "|  18   | 1980 | Rebecca   | 757        |\r\n",
      "|  19   | 1980 | Laura     | 756        |\r\n",
      "|-------+------+-----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvstack name_a.csv name_b.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use an optional flag -g to add a grouping column to indicate which file each row came from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--------+------+------+-----------+------------|\r\n",
      "|  group | Rank | Year | Name      | Frequency  |\r\n",
      "|--------+------+------+-----------+------------|\r\n",
      "|  a     | 1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  a     | 2    | 1980 | Amanda    | 1750       |\r\n",
      "|  a     | 3    | 1980 | Melissa   | 1566       |\r\n",
      "|  a     | 4    | 1980 | Sarah     | 1390       |\r\n",
      "|  a     | 5    | 1980 | Jessica   | 1373       |\r\n",
      "|  a     | 6    | 1980 | Nicole    | 1336       |\r\n",
      "|  a     | 7    | 1980 | Elizabeth | 1221       |\r\n",
      "|  a     | 8    | 1980 | Michelle  | 1170       |\r\n",
      "|  a     | 9    | 1980 | Amy       | 1013       |\r\n",
      "|  b     | 10   | 1980 | Tiffany   | 941        |\r\n",
      "|  b     | 11   | 1980 | Angela    | 924        |\r\n",
      "|  b     | 12   | 1980 | Kelly     | 895        |\r\n",
      "|  b     | 13   | 1980 | Heather   | 893        |\r\n",
      "|  b     | 14   | 1980 | Kimberly  | 854        |\r\n",
      "|  b     | 15   | 1980 | Lisa      | 845        |\r\n",
      "|  b     | 16   | 1980 | Stephanie | 839        |\r\n",
      "|  b     | 17   | 1980 | Christina | 778        |\r\n",
      "|  b     | 18   | 1980 | Rebecca   | 757        |\r\n",
      "|  b     | 19   | 1980 | Laura     | 756        |\r\n",
      "|--------+------+------+-----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvstack -g a,b name_a.csv name_b.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use csvkit elsewhere\n",
    "The command line may not be enough when solving some problems. In this section, we will see how csvkit collaborates with other tools to make it more powerful.\n",
    "\n",
    "### 3.1. sql\n",
    "csvsql and sql2csv are bridges connecting the CSV file and the SQL database.\n",
    "### csvsql\n",
    "csvsql generates SQL statement for a CSV file or executes those statements on a database.  \n",
    "\n",
    "Let's save the stacked rows above into a new CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!csvstack -g a,b name_a.csv name_b.csv > stacked.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a create table statement for stacked.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE stacked (\r\n",
      "\t\"group\" VARCHAR(1) NOT NULL, \r\n",
      "\t\"Rank\" INTEGER NOT NULL, \r\n",
      "\t\"Year\" INTEGER NOT NULL, \r\n",
      "\t\"Name\" VARCHAR(9) NOT NULL, \r\n",
      "\t\"Frequency\" INTEGER NOT NULL\r\n",
      ");\r\n"
     ]
    }
   ],
   "source": [
    "!csvsql -i sqlite stacked.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a table and import data from stacked.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sqlite3.OperationalError) table stacked already exists [SQL: u'\\nCREATE TABLE stacked (\\n\\t\"group\" VARCHAR(1) NOT NULL, \\n\\t\"Rank\" INTEGER NOT NULL, \\n\\t\"Year\" INTEGER NOT NULL, \\n\\t\"Name\" VARCHAR(9) NOT NULL, \\n\\t\"Frequency\" INTEGER NOT NULL\\n)\\n\\n']\r\n"
     ]
    }
   ],
   "source": [
    "!csvsql --db sqlite:///dummy.db --insert stacked.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql2csv\n",
    "sql2csv executes arbitrary commands against a SQL database and outputs the results as a CSV file.  \n",
    "\n",
    "We can use sql2csv to check the data we just imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--------+------+------+-----------+------------|\r\n",
      "|  group | Rank | Year | Name      | Frequency  |\r\n",
      "|--------+------+------+-----------+------------|\r\n",
      "|  a     | 1    | 1980 | Jennifer  | 3017       |\r\n",
      "|  a     | 2    | 1980 | Amanda    | 1750       |\r\n",
      "|  a     | 3    | 1980 | Melissa   | 1566       |\r\n",
      "|  a     | 4    | 1980 | Sarah     | 1390       |\r\n",
      "|  a     | 5    | 1980 | Jessica   | 1373       |\r\n",
      "|  a     | 6    | 1980 | Nicole    | 1336       |\r\n",
      "|  a     | 7    | 1980 | Elizabeth | 1221       |\r\n",
      "|  a     | 8    | 1980 | Michelle  | 1170       |\r\n",
      "|  a     | 9    | 1980 | Amy       | 1013       |\r\n",
      "|  b     | 10   | 1980 | Tiffany   | 941        |\r\n",
      "|  b     | 11   | 1980 | Angela    | 924        |\r\n",
      "|  b     | 12   | 1980 | Kelly     | 895        |\r\n",
      "|  b     | 13   | 1980 | Heather   | 893        |\r\n",
      "|  b     | 14   | 1980 | Kimberly  | 854        |\r\n",
      "|  b     | 15   | 1980 | Lisa      | 845        |\r\n",
      "|  b     | 16   | 1980 | Stephanie | 839        |\r\n",
      "|  b     | 17   | 1980 | Christina | 778        |\r\n",
      "|  b     | 18   | 1980 | Rebecca   | 757        |\r\n",
      "|  b     | 19   | 1980 | Laura     | 756        |\r\n",
      "|--------+------+------+-----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!sql2csv --db sqlite:///dummy.db --query \"select * from stacked\" | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first rank from the stacked table in sqlite database, we could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--------+------+------+----------+------------|\r\n",
      "|  group | Rank | Year | Name     | Frequency  |\r\n",
      "|--------+------+------+----------+------------|\r\n",
      "|  a     | 1    | 1980 | Jennifer | 3017       |\r\n",
      "|--------+------+------+----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!sql2csv --db sqlite:///dummy.db --query \"select * from stacked where rank=1;\" | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even entirely skip the database because csvsql will create one in memory for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--------+------+------+----------+------------|\r\n",
      "|  group | Rank | Year | Name     | Frequency  |\r\n",
      "|--------+------+------+----------+------------|\r\n",
      "|  a     | 1    | 1980 | Jennifer | 3017       |\r\n",
      "|--------+------+------+----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvsql --query \"select * from stacked where rank=1;\" stacked.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want something more interesting?  \n",
    "\n",
    "Extract all the Rank 1 row from name.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+----------+------------|\r\n",
      "|  Rank | Year | Name     | Frequency  |\r\n",
      "|-------+------+----------+------------|\r\n",
      "|  1    | 1980 | Jennifer | 3017       |\r\n",
      "|  1    | 1981 | Jennifer | 2920       |\r\n",
      "|  1    | 1982 | Jennifer | 2809       |\r\n",
      "|  1    | 1983 | Jennifer | 2636       |\r\n",
      "|  1    | 1984 | Jennifer | 2528       |\r\n",
      "|  1    | 1985 | Jennifer | 2096       |\r\n",
      "|  1    | 1986 | Jessica  | 2198       |\r\n",
      "|  1    | 1987 | Ashley   | 2565       |\r\n",
      "|  1    | 1988 | Jessica  | 2275       |\r\n",
      "|  1    | 1989 | Ashley   | 2308       |\r\n",
      "|  1    | 1990 | Jessica  | 2117       |\r\n",
      "|  1    | 1991 | Jessica  | 2075       |\r\n",
      "|  1    | 1992 | Jessica  | 1816       |\r\n",
      "|  1    | 1993 | Jessica  | 1789       |\r\n",
      "|  1    | 1994 | Jessica  | 1640       |\r\n",
      "|  1    | 1995 | Jessica  | 1429       |\r\n",
      "|  1    | 1996 | Jessica  | 1265       |\r\n",
      "|  1    | 1997 | Emily    | 1213       |\r\n",
      "|  1    | 1998 | Emily    | 1211       |\r\n",
      "|  1    | 1999 | Emily    | 1210       |\r\n",
      "|  1    | 2000 | Emily    | 1205       |\r\n",
      "|  1    | 2001 | Emily    | 1182       |\r\n",
      "|  1    | 2002 | Emily    | 1117       |\r\n",
      "|  1    | 2003 | Emily    | 1132       |\r\n",
      "|  1    | 2004 | Emily    | 1094       |\r\n",
      "|  1    | 2005 | Emily    | 1015       |\r\n",
      "|  1    | 2006 | EMILY    | 937        |\r\n",
      "|  1    | 2007 | ISABELLA | 893        |\r\n",
      "|  1    | 2008 | OLIVIA   | 850        |\r\n",
      "|  1    | 2009 | ISABELLA | 971        |\r\n",
      "|  1    | 2010 | ISABELLA | 940        |\r\n",
      "|  1    | 2011 | SOPHIA   | 890        |\r\n",
      "|  1    | 2012 | SOPHIA   | 957        |\r\n",
      "|  1    | 2013 | OLIVIA   | 856        |\r\n",
      "|-------+------+----------+------------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvgrep -c 1 -r \"^1$\" name.csv > rank1.csv\n",
    "!csvlook rank1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the frequency of all the names in rank1.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,s\r\n",
      "Jessica,16604\r\n",
      "Jennifer,16006\r\n",
      "Emily,10379\r\n",
      "Ashley,4873\r\n",
      "ISABELLA,2804\r\n",
      "SOPHIA,1847\r\n",
      "OLIVIA,1706\r\n",
      "EMILY,937\r\n"
     ]
    }
   ],
   "source": [
    "!csvsql --query \"select Name, sum(Frequency) as s from rank1 group by Name order by s DESC;\" rank1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use asciigraph to make things more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16604  Jessica \r\n",
      "     16006  Jennifer\r\n",
      "                          10379  Emily   \r\n",
      "                                               4873  Ashley  \r\n",
      "                                                       2804  ISABELLA\r\n",
      "                                                           1847  SOPHIA  \r\n",
      "                                                           1706  OLIVIA  \r\n",
      "                                                               937  EMILY   \r\n"
     ]
    }
   ],
   "source": [
    "!csvsql --query \"select Name, sum(Frequency) as s from rank1 group by Name order by s DESC;\" rank1.csv | \\\n",
    "csvformat -D \":\" | tail -n +2 | asciigraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. json\n",
    "### csvjson\n",
    "csvjson converts a CSV file into JSON or GeoJSON.  \n",
    "\n",
    "Let's make a smaller slice from stacked.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+------+-----------|\r\n",
      "|  Rank | Year | Name      |\r\n",
      "|-------+------+-----------|\r\n",
      "|  1    | 1980 | Jennifer  |\r\n",
      "|  2    | 1980 | Amanda    |\r\n",
      "|  3    | 1980 | Melissa   |\r\n",
      "|-------+------+-----------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c 2,3,4 stacked.csv | csvgrep -c 1 -r \"^[1-3]$\" | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert that small slice into JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "    {\r\n",
      "        \"Rank\": \"1\", \r\n",
      "        \"Year\": \"1980\", \r\n",
      "        \"Name\": \"Jennifer\"\r\n",
      "    }, \r\n",
      "    {\r\n",
      "        \"Rank\": \"2\", \r\n",
      "        \"Year\": \"1980\", \r\n",
      "        \"Name\": \"Amanda\"\r\n",
      "    }, \r\n",
      "    {\r\n",
      "        \"Rank\": \"3\", \r\n",
      "        \"Year\": \"1980\", \r\n",
      "        \"Name\": \"Melissa\"\r\n",
      "    }\r\n",
      "]"
     ]
    }
   ],
   "source": [
    "!csvcut -c 2,3,4 stacked.csv | csvgrep -c 1 -r \"^[1-3]$\" | csvjson --indent 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide a column name as the key to the JSON and all the values in the column must be unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"1\": {\r\n",
      "        \"Rank\": \"1\", \r\n",
      "        \"Year\": \"1980\", \r\n",
      "        \"Name\": \"Jennifer\"\r\n",
      "    }, \r\n",
      "    \"2\": {\r\n",
      "        \"Rank\": \"2\", \r\n",
      "        \"Year\": \"1980\", \r\n",
      "        \"Name\": \"Amanda\"\r\n",
      "    }, \r\n",
      "    \"3\": {\r\n",
      "        \"Rank\": \"3\", \r\n",
      "        \"Year\": \"1980\", \r\n",
      "        \"Name\": \"Melissa\"\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!csvcut -c 2,3,4 stacked.csv | csvgrep -c 1 -r \"^[1-3]$\" | csvjson --indent 4 --key Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. python\n",
    "### csvpy\n",
    "csvpy loads a CSV file into a reader object and then drops into a Python shell.  \n",
    "\n",
    "Invoking csvpy would launch a Python terminal: \n",
    "\n",
    ">$ csvpy stacked.csv  \n",
    "\n",
    "```python\n",
    ">>>reader.next()  \n",
    "[u'group', u'Rank', u'Year', u'Name', u'Frequency']\n",
    "```\n",
    "\n",
    "We can also use a CSV DictReader instead:  \n",
    "\n",
    ">$ csvpy --dict stacked.csv -v  \n",
    "\n",
    "```python\n",
    ">>>reader.next()\n",
    "{u'Frequency': u'3017',\n",
    " u'Name': u'Jennifer',\n",
    " u'Rank': u'1',\n",
    " u'Year': u'1980',\n",
    " u'group': u'a'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use csvkit as a Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the command-line usage, we can **import csvkit** to use it as a Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csvkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read a CSV file using CSVKitReader() and CSVKitDictReader():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'group', u'Rank', u'Year', u'Name', u'Frequency']\n",
      "[u'a', u'1', u'1980', u'Jennifer', u'3017']\n",
      "[u'a', u'2', u'1980', u'Amanda', u'1750']\n",
      "[u'a', u'3', u'1980', u'Melissa', u'1566']\n",
      "[u'a', u'4', u'1980', u'Sarah', u'1390']\n",
      "[u'a', u'5', u'1980', u'Jessica', u'1373']\n",
      "[u'a', u'6', u'1980', u'Nicole', u'1336']\n",
      "[u'a', u'7', u'1980', u'Elizabeth', u'1221']\n",
      "[u'a', u'8', u'1980', u'Michelle', u'1170']\n",
      "[u'a', u'9', u'1980', u'Amy', u'1013']\n"
     ]
    }
   ],
   "source": [
    "with open('stacked.csv', 'rb') as f:\n",
    "    reader = csvkit.CSVKitReader(f)\n",
    "    for i in xrange(10):\n",
    "        print reader.next()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'group', u'Rank', u'Year', u'Name', u'Frequency']\n",
      "{u'Frequency': u'3017', u'group': u'a', u'Name': u'Jennifer', u'Rank': u'1', u'Year': u'1980'}\n",
      "{u'Frequency': u'1750', u'group': u'a', u'Name': u'Amanda', u'Rank': u'2', u'Year': u'1980'}\n",
      "{u'Frequency': u'1566', u'group': u'a', u'Name': u'Melissa', u'Rank': u'3', u'Year': u'1980'}\n",
      "{u'Frequency': u'1390', u'group': u'a', u'Name': u'Sarah', u'Rank': u'4', u'Year': u'1980'}\n",
      "{u'Frequency': u'1373', u'group': u'a', u'Name': u'Jessica', u'Rank': u'5', u'Year': u'1980'}\n",
      "{u'Frequency': u'1336', u'group': u'a', u'Name': u'Nicole', u'Rank': u'6', u'Year': u'1980'}\n",
      "{u'Frequency': u'1221', u'group': u'a', u'Name': u'Elizabeth', u'Rank': u'7', u'Year': u'1980'}\n",
      "{u'Frequency': u'1170', u'group': u'a', u'Name': u'Michelle', u'Rank': u'8', u'Year': u'1980'}\n",
      "{u'Frequency': u'1013', u'group': u'a', u'Name': u'Amy', u'Rank': u'9', u'Year': u'1980'}\n",
      "{u'Frequency': u'941', u'group': u'b', u'Name': u'Tiffany', u'Rank': u'10', u'Year': u'1980'}\n"
     ]
    }
   ],
   "source": [
    "with open('stacked.csv', 'rb') as f:\n",
    "    reader = csvkit.CSVKitDictReader(f)\n",
    "    print reader.fieldnames\n",
    "    for i in xrange(10):\n",
    "        print reader.next()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write data into a CSV file. Like CSVKitReader() and CSVKitDictReader(), we can use CSVKitWriter() and CSVKitDictWriter(). For example, let's create an activity plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date Day of Week  Activity\n",
      "0  20161101     Tuesday   Running\n",
      "1  20161102   Wednesday   Jogging\n",
      "2  20161103    Thursday   Running\n",
      "3  20161104      Friday   Running\n",
      "4  20161105    Saturday   Jogging\n",
      "5  20161106      Sunday   Jogging\n",
      "6  20161107      Monday   Jogging\n",
      "7  20161108     Tuesday  Swimming\n",
      "8  20161109   Wednesday   Running\n",
      "9  20161110    Thursday  Swimming\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "with open('activity.csv', 'wb') as f:\n",
    "    writer = csvkit.CSVKitWriter(f)\n",
    "    writer.writerow(['Date', 'Day of Week', 'Activity'])\n",
    "    start_date = 20161101\n",
    "    act_set = ['Running', 'Swimming', 'Jogging', 'Walking']\n",
    "    for i in xrange(10):\n",
    "        data_date = str(start_date + i)\n",
    "        data_date_dt = datetime.datetime.strptime(data_date, '%Y%m%d')\n",
    "        data_day_of_week = calendar.day_name[data_date_dt.weekday()]\n",
    "        data_act = act_set[random.randint(0, 2)]\n",
    "        writer.writerow([data_date, data_day_of_week, data_act])\n",
    "f.close()\n",
    "df = pd.read_csv('activity.csv')\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using CSVKitDictWriter(), we need to specify the fieldnames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date Day of Week Activity\n",
      "0  20161101     Tuesday  Reading\n",
      "1  20161102   Wednesday  Reading\n",
      "2  20161103    Thursday  Singing\n",
      "3  20161104      Friday  Dancing\n",
      "4  20161105    Saturday  Reading\n",
      "5  20161106      Sunday  Reading\n",
      "6  20161107      Monday  Reading\n",
      "7  20161108     Tuesday  Singing\n",
      "8  20161109   Wednesday  Reading\n",
      "9  20161110    Thursday  Dancing\n"
     ]
    }
   ],
   "source": [
    "with open('activity_dict.csv', 'wb') as f:\n",
    "    fields = ['Date', 'Day of Week', 'Activity']\n",
    "    writer = csvkit.CSVKitDictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader() # don't forget this line!\n",
    "    start_date = 20161101\n",
    "    act_set = ['Singing', 'Dancing', 'Reading']\n",
    "    for i in xrange(10):\n",
    "        dct = {}\n",
    "        data_date = str(start_date + i)\n",
    "        dct[fields[0]] = data_date\n",
    "        data_date_dt = datetime.datetime.strptime(data_date, '%Y%m%d')\n",
    "        data_day_of_week = calendar.day_name[data_date_dt.weekday()]\n",
    "        dct[fields[1]] = data_day_of_week\n",
    "        data_act = act_set[random.randint(0, 2)]\n",
    "        dct[fields[2]] = data_act\n",
    "        writer.writerow(dct)\n",
    "f.close()\n",
    "df = pd.read_csv('activity_dict.csv')\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and references\n",
    "This tutorial introduces how to use csvkit as command-line tools and as a Python library. If you want to explore more about csvkit, you could:\n",
    "\n",
    "1. Go to [documentation](http://csvkit.readthedocs.io/en/540/index.html) for more details about the usage of csvkit.  \n",
    "2. Go to [DATA.GOV](https://catalog.data.gov/dataset) for more real datasets of a variety of topics."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
