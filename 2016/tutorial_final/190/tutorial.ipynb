{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analytics with Spark\n",
    "\n",
    "Apache Spark is an open source cluster computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.(From wikipedia)\n",
    "\n",
    "This tutorial will guide you to install Spark and run some classic big data programs based on Spark. We will re-implement some functions that we already implemented in former homework based on Spark.\n",
    "\n",
    "Please note that Spark will store data in the memory, and if the test data is too big, some problems may happen. So, the test dataset is only 100 lines, but it will be pretty easy to scale-up.\n",
    "\n",
    "## Q0: Install Spark\n",
    "### For Mac\n",
    "#### Install brew\n",
    "\n",
    "```bash\n",
    "ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
    "```\n",
    "\n",
    "#### Install Spark 1.6 via brew\n",
    "\n",
    "```bash\n",
    "brew install homebrew/versions/apache-spark16\n",
    "```\n",
    "\n",
    "#### Link Spark with Jupyter Notebook\n",
    "Add the following ENV_VAR:\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "```\n",
    "\n",
    "#### Test Spark with example\n",
    "\n",
    "```bash\n",
    "run-example org.apache.spark.examples.SparkPi\n",
    "```\n",
    "\n",
    "#### Start notebook with Spark\n",
    "```bash\n",
    "pyspark --packages graphframes:graphframes:0.1.0-spark1.6 --executor-memory 4g --driver-memory 4g\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/10/19 00:41:01 INFO SparkContext: Running Spark version 1.6.2\n",
      "16/10/19 00:41:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/19 00:41:06 INFO SecurityManager: Changing view acls to: yuanchen\n",
      "16/10/19 00:41:06 INFO SecurityManager: Changing modify acls to: yuanchen\n",
      "16/10/19 00:41:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yuanchen); users with modify permissions: Set(yuanchen)\n",
      "16/10/19 00:41:12 INFO Utils: Successfully started service 'sparkDriver' on port 63326.\n",
      "16/10/19 00:41:12 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/10/19 00:41:12 INFO Remoting: Starting remoting\n",
      "16/10/19 00:41:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.3.30:63327]\n",
      "16/10/19 00:41:12 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 63327.\n",
      "16/10/19 00:41:13 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/10/19 00:41:13 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/10/19 00:41:13 INFO DiskBlockManager: Created local directory at /private/var/folders/xf/977802bn1gl4ydj9bhs2mp2c0000gn/T/blockmgr-1ff4b85b-c9a5-4cf9-a22d-b7f44fe75e7b\n",
      "16/10/19 00:41:13 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "16/10/19 00:41:13 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/10/19 00:41:13 WARN QueuedThreadPool: 1 threads could not be stopped\n",
      "16/10/19 00:41:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "16/10/19 00:41:13 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "16/10/19 00:41:13 INFO SparkUI: Started SparkUI at http://192.168.3.30:4041\n",
      "16/10/19 00:41:13 INFO HttpFileServer: HTTP File server directory is /private/var/folders/xf/977802bn1gl4ydj9bhs2mp2c0000gn/T/spark-baafd1d0-d4fd-4b3f-906e-8deab9974f7e/httpd-f9553aae-94ac-4971-bfe0-6c51caf1eb19\n",
      "16/10/19 00:41:13 INFO HttpServer: Starting HTTP Server\n",
      "16/10/19 00:41:13 INFO Utils: Successfully started service 'HTTP file server' on port 63328.\n",
      "16/10/19 00:41:13 INFO SparkContext: Added JAR file:/usr/local/Cellar/apache-spark16/1.6.2/libexec/lib/spark-examples-1.6.2-hadoop2.6.0.jar at http://192.168.3.30:63328/jars/spark-examples-1.6.2-hadoop2.6.0.jar with timestamp 1476862873762\n",
      "16/10/19 00:41:13 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/10/19 00:41:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63329.\n",
      "16/10/19 00:41:13 INFO NettyBlockTransferService: Server created on 63329\n",
      "16/10/19 00:41:13 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/10/19 00:41:13 INFO BlockManagerMasterEndpoint: Registering block manager localhost:63329 with 511.1 MB RAM, BlockManagerId(driver, localhost, 63329)\n",
      "16/10/19 00:41:13 INFO BlockManagerMaster: Registered BlockManager\n",
      "16/10/19 00:41:14 INFO SparkContext: Starting job: reduce at SparkPi.scala:36\n",
      "16/10/19 00:41:14 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:36) with 2 output partitions\n",
      "16/10/19 00:41:14 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:36)\n",
      "16/10/19 00:41:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "16/10/19 00:41:14 INFO DAGScheduler: Missing parents: List()\n",
      "16/10/19 00:41:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32), which has no missing parents\n",
      "16/10/19 00:41:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1904.0 B, free 1904.0 B)\n",
      "16/10/19 00:41:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1218.0 B, free 3.0 KB)\n",
      "16/10/19 00:41:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:63329 (size: 1218.0 B, free: 511.1 MB)\n",
      "16/10/19 00:41:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006\n",
      "16/10/19 00:41:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32)\n",
      "16/10/19 00:41:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "16/10/19 00:41:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2155 bytes)\n",
      "16/10/19 00:41:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2155 bytes)\n",
      "16/10/19 00:41:14 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "16/10/19 00:41:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "16/10/19 00:41:14 INFO Executor: Fetching http://192.168.3.30:63328/jars/spark-examples-1.6.2-hadoop2.6.0.jar with timestamp 1476862873762\n",
      "16/10/19 00:41:14 INFO Utils: Fetching http://192.168.3.30:63328/jars/spark-examples-1.6.2-hadoop2.6.0.jar to /private/var/folders/xf/977802bn1gl4ydj9bhs2mp2c0000gn/T/spark-baafd1d0-d4fd-4b3f-906e-8deab9974f7e/userFiles-9a3e5701-f58d-45cb-afe2-cee7244498de/fetchFileTemp8614898384687902848.tmp\n",
      "16/10/19 00:41:15 INFO Executor: Adding file:/private/var/folders/xf/977802bn1gl4ydj9bhs2mp2c0000gn/T/spark-baafd1d0-d4fd-4b3f-906e-8deab9974f7e/userFiles-9a3e5701-f58d-45cb-afe2-cee7244498de/spark-examples-1.6.2-hadoop2.6.0.jar to class loader\n",
      "16/10/19 00:41:15 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1031 bytes result sent to driver\n",
      "16/10/19 00:41:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1031 bytes result sent to driver\n",
      "16/10/19 00:41:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 639 ms on localhost (1/2)\n",
      "16/10/19 00:41:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 669 ms on localhost (2/2)\n",
      "16/10/19 00:41:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "16/10/19 00:41:15 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 0.688 s\n",
      "16/10/19 00:41:15 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 0.909981 s\n",
      "Pi is roughly 3.14032\n",
      "16/10/19 00:41:15 INFO SparkUI: Stopped Spark web UI at http://192.168.3.30:4041\n",
      "16/10/19 00:41:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "16/10/19 00:41:15 INFO MemoryStore: MemoryStore cleared\n",
      "16/10/19 00:41:15 INFO BlockManager: BlockManager stopped\n",
      "16/10/19 00:41:15 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "16/10/19 00:41:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "16/10/19 00:41:15 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "16/10/19 00:41:15 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n",
      "16/10/19 00:41:15 INFO SparkContext: Successfully stopped SparkContext\n",
      "16/10/19 00:41:15 INFO ShutdownHookManager: Shutdown hook called\n",
      "16/10/19 00:41:15 INFO ShutdownHookManager: Deleting directory /private/var/folders/xf/977802bn1gl4ydj9bhs2mp2c0000gn/T/spark-baafd1d0-d4fd-4b3f-906e-8deab9974f7e\n",
      "16/10/19 00:41:15 INFO ShutdownHookManager: Deleting directory /private/var/folders/xf/977802bn1gl4ydj9bhs2mp2c0000gn/T/spark-baafd1d0-d4fd-4b3f-906e-8deab9974f7e/httpd-f9553aae-94ac-4971-bfe0-6c51caf1eb19\n"
     ]
    }
   ],
   "source": [
    "p = subprocess.Popen('run-example org.apache.spark.examples.SparkPi',\\\n",
    "                     shell=True, stdout=subprocess.PIPE, \\\n",
    "                     stderr=subprocess.STDOUT)\n",
    "for line in p.stdout.readlines():\n",
    "    print line,\n",
    "retval = p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following output indicates the installation is successful\n",
    "```\n",
    "    16/10/18 19:16:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1031 bytes result sent to driver\n",
    "    16/10/18 19:16:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1031 bytes result sent to driver\n",
    "    16/10/18 19:16:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 512 ms on localhost (1/2)\n",
    "    16/10/18 19:16:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 543 ms on localhost (2/2)\n",
    "    16/10/18 19:16:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
    "    16/10/18 19:16:13 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 0.558 s\n",
    "    16/10/18 19:16:13 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 0.745534 s\n",
    "    Pi is roughly 3.14766\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import add\n",
    "from graphframes import *\n",
    "import re\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up: write your own Pi Estimation\n",
    "To get more familiar with Spark's map and reduce function, we will finished the following code for Pi Estimation. The general idea to estimates Pi is to \"throwing darts\" at a circle. It is called Monte Carlo integration  \n",
    "https://en.wikipedia.org/wiki/Monte_Carlo_integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be Pi / 4, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.232\n"
     ]
    }
   ],
   "source": [
    "def sample(p):\n",
    "    \"\"\"\n",
    "    return the result of one sample\n",
    "    \"\"\"\n",
    "    x, y = random(), random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "def add(a,b):\n",
    "    \"\"\"\n",
    "    return the sum of two values\n",
    "    \n",
    "    actually add is a buildin operator, you do not actually need to implement this.\n",
    "    add this code to help student understand how to write function for reducer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "#generate 1000 smaples\n",
    "experiment_index = sc.parallelize(xrange(0, 1000))\n",
    "#using map and smaple function to get experimental value of samples\n",
    "experiment_result = experiment_index.map(sample)\n",
    "#reduce to get the result \n",
    "\n",
    "experiment_count = experiment_result.reduce(add)\n",
    "\n",
    "print 4.0 * experiment_count / 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Implement Word Count\n",
    "Word count is a classic example of Map-Reduce. We can also use spark to do the word count.\n",
    "\n",
    "In the map part we map every word in the text file into tuple (word,1)\n",
    "\n",
    "In the reduce part, we reduce every tuple and add the value to get the final count of the word\n",
    "\n",
    "Sparkâ€™s API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:\n",
    "\n",
    "- Local defs inside the function calling into Spark, for longer code.\n",
    "- Lambda expressions, for simple functions that can be written as an expression. (Lambdas do not support multi-statement functions or statements that do not return a value.)\n",
    "\n",
    "We will try these two ways in the following part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 Word Count Implementation\n",
    "The following function is implemented to pass to Spark, it will be suitable for long code\n",
    "- split_line: In this function you will need to split input line to words\n",
    "- word_pair: In this function, you will need to generate word pair (word,1)\n",
    "- count: In this function, you will need to add the count of each word pair\n",
    "\n",
    "In this, we will using map, flatmap and reduce and above functions to get the word_count\n",
    "- word_count: In this function, the above 3 functions will be used in this function and generate the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    \"\"\" Split line in to words and get only alphanumeric words\n",
    "    Args: \n",
    "        line (string): an input line\n",
    "    Returns:\n",
    "        list: a list of words\n",
    "    \"\"\"\n",
    "    words = line.split()\n",
    "    words = [re.sub(r'\\W+', '', word) for word in words]\n",
    "    res = filter(lambda word: re.match(r'\\w+', word), words)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['realDonaldTrump', 'Trump']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"realDonaldTrump Trump\"\n",
    "split_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_pair(word):\n",
    "    \"\"\" generate word pair like (word, 1)\n",
    "    Args: \n",
    "        word (string): an input word\n",
    "    Returns:\n",
    "        tuple: a word pair\n",
    "    \"\"\"\n",
    "    return (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('realDonaldTrump', 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"realDonaldTrump\"\n",
    "word_pair(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(a, b):\n",
    "    \"\"\" generate word pair like (word, 1)\n",
    "    Args: \n",
    "        a (int): an input count\n",
    "        b (int): an input count\n",
    "    Returns:\n",
    "        int : result count\n",
    "    \"\"\"\n",
    "    return (a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "count(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(path):\n",
    "    \"\"\" Count the word in a text file using spark, using flatMap, map and reduceByKey function\n",
    "    Args: \n",
    "        path (string): path to the file\n",
    "    Returns:\n",
    "        dict: a dict that word is the key and the count is the value\n",
    "    \"\"\"\n",
    "    text_file = sc.textFile(path)\n",
    "    words = text_file.flatMap(split_line)\n",
    "    pairs = words.map(word_pair)\n",
    "    counts = pairs.reduceByKey(count)\n",
    "    count_dict = dict(counts.collect())\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "path = \"edges.csv\"\n",
    "d = word_count(path)\n",
    "print d['TrumpGolf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 Word Count Implementation With Lambda function\n",
    "We can also using lambda functions in the Spark, but we need to notice that lambdas do not support multi-statement functions or statements that do not return a value.\n",
    "But most of times, in each step, the code is pretty simple and using lambda seems a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_count(path):\n",
    "    \"\"\" Count the word in a text file using spark\n",
    "    Args: \n",
    "        path (string): path to the file\n",
    "        \n",
    "    Returns:\n",
    "        dict: a dict that word is the key and the count is the value\n",
    "    \"\"\"\n",
    "    text_file = sc.textFile(path)\n",
    "    count = text_file.flatMap(lambda line: line.split(' '))\\\n",
    "                    .map(lambda word: (word, 1))\\\n",
    "                    .reduceByKey(lambda a, b: a+b)\n",
    "    count_dict = dict(count.collect())\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "path = \"edges.csv\"\n",
    "d = word_count(path)\n",
    "print d['TrumpGolf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Calculate PageRank Using GraphFrames\n",
    "\n",
    "We learned how to calculate PageRank in Homework 2, and we all noticed that, when we try to calculating a large matrix, the whole process seems really slow. But what if we want to scale up?\n",
    "\n",
    "Spark may be the answer.\n",
    "\n",
    "https://github.com/graphframes/graphframes\n",
    "\n",
    "Since Python API has not implemented in GraphX, so we use GraphFrames, which warps GraphX algorithms.\n",
    "\n",
    "We first load data from \"edges.csv\" file to a Vertex DataFrame with unique ID column 'id'\n",
    "\n",
    "And then load data from \"deges.csv\" file and create an edge DataFrame with 'src' and 'dst' colums.\n",
    "\n",
    "And create a GraphFrame based on Vertex and Edge\n",
    "\n",
    "Finally use buildin function to get the pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    \"\"\" Load text file into RDD\n",
    "    Args: \n",
    "        path (string): path to the file\n",
    "    Returns:\n",
    "        MapPartitionsRDD\n",
    "    \"\"\"\n",
    "    text_file = sc.textFile(path)\n",
    "    return text_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code should get the result:\n",
    "    \n",
    "    [u'realDonaldTrump Trump', u'realDonaldTrump TrumpGolf', u'realDonaldTrump TiffanyATrump', u'realDonaldTrump IngrahamAngle', u'realDonaldTrump mike_pence', u'realDonaldTrump TeamTrump', u'realDonaldTrump DRUDGE_REPORT', u'realDonaldTrump MrsVanessaTrump', u'realDonaldTrump LaraLeaTrump', u'realDonaldTrump seanhannity', u'realDonaldTrump foxnation', u'realDonaldTrump CLewandowski_', u'realDonaldTrump AnnCoulter', u'realDonaldTrump DiamondandSilk', u'realDonaldTrump KatrinaCampins', u'realDonaldTrump KatrinaPierson', u'realDonaldTrump MichaelCohen212', u'realDonaldTrump foxandfriends', u'realDonaldTrump MELANIATRUMP', u'realDonaldTrump GeraldoRivera', u'realDonaldTrump ericbolling', u'realDonaldTrump RealRomaDowney', u'realDonaldTrump MarkBurnettTV', u'realDonaldTrump garyplayer', u'realDonaldTrump MagicJohnson', u'realDonaldTrump VinceMcMahon', u'realDonaldTrump DanScavino', u'realDonaldTrump TrumpWaikiki', u'realDonaldTrump TrumpDoral', u'realDonaldTrump TrumpCharlotte', u'realDonaldTrump TrumpLasVegas', u'realDonaldTrump TrumpChicago', u'realDonaldTrump TrumpGolfDC', u'realDonaldTrump TrumpGolfLA', u'realDonaldTrump EricTrump', u'realDonaldTrump morningmika', u'realDonaldTrump JoeNBC'...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'realDonaldTrump Trump', u'realDonaldTrump TrumpGolf', u'realDonaldTrump TiffanyATrump', u'realDonaldTrump IngrahamAngle', u'realDonaldTrump mike_pence', u'realDonaldTrump TeamTrump', u'realDonaldTrump DRUDGE_REPORT', u'realDonaldTrump MrsVanessaTrump', u'realDonaldTrump LaraLeaTrump', u'realDonaldTrump seanhannity', u'realDonaldTrump foxnation', u'realDonaldTrump CLewandowski_', u'realDonaldTrump AnnCoulter', u'realDonaldTrump DiamondandSilk', u'realDonaldTrump KatrinaCampins', u'realDonaldTrump KatrinaPierson', u'realDonaldTrump MichaelCohen212', u'realDonaldTrump foxandfriends', u'realDonaldTrump MELANIATRUMP', u'realDonaldTrump GeraldoRivera', u'realDonaldTrump ericbolling', u'realDonaldTrump RealRomaDowney', u'realDonaldTrump MarkBurnettTV', u'realDonaldTrump garyplayer', u'realDonaldTrump MagicJohnson', u'realDonaldTrump VinceMcMahon', u'realDonaldTrump DanScavino', u'realDonaldTrump TrumpWaikiki', u'realDonaldTrump TrumpDoral', u'realDonaldTrump TrumpCharlotte', u'realDonaldTrump TrumpLasVegas', u'realDonaldTrump TrumpChicago', u'realDonaldTrump TrumpGolfDC', u'realDonaldTrump TrumpGolfLA', u'realDonaldTrump EricTrump', u'realDonaldTrump morningmika', u'realDonaldTrump JoeNBC', u'realDonaldTrump oreillyfactor', u'realDonaldTrump greta', u'realDonaldTrump piersmorgan', u'realDonaldTrump DonaldJTrumpJr', u'realDonaldTrump IvankaTrump', u'Trump TiffanyATrump', u'Trump TrumpJupiter', u'Trump TrumpPalmBeach', u'Trump TrumpGolf', u'Trump TrumpTower', u'Trump trumpwinery', u'Trump TrumpTurnberry', u'Trump TrumpBedminster', u'Trump TrumpNationalNY', u'Trump TrumpScotland', u'Trump TrumpWaikiki', u'Trump TrumpDoonbeg', u'Trump TrumpPanama', u'Trump TrumpToronto', u'Trump TrumpHotels', u'Trump TrumpChicago', u'Trump TrumpLasVegas', u'Trump TrumpGolfPhilly', u'Trump TrumpColtsNeck', u'Trump TrumpGolfHV', u'Trump TrumpDC', u'Trump TrumpGolfDC', u'Trump TrumpGolfLA', u'Trump TrumpDoral', u'Trump TrumpCharlotte', u'Trump TrumpRealty', u'Trump TrumpSoHo', u'Trump TrumpNewYork', u'Trump EricTrumpFdn', u'Trump TrumpFerryPoint', u'Trump TrumpModels', u'Trump DonaldJTrumpJr', u'Trump EricTrump', u'Trump IvankaTrump', u'Trump realDonaldTrump', u'TrumpGolf MichaelBreed', u'TrumpGolf Trump', u'TrumpGolf TrumpJupiter', u'TrumpGolf MichaelBreedGA', u'TrumpGolf BrendanAMurphy', u'TrumpGolf CallawayGolf', u'TrumpGolf PGA_JohnDaly', u'TrumpGolf SIGolfPlus', u'TrumpGolf parscale', u'TrumpGolf horsebalmedie', u'TrumpGolf livelasvegas', u'TrumpGolf SMURFpga', u'TrumpGolf CRTurnberry', u'TrumpGolf Cmiddaughgolf', u'TrumpGolf joshspragins', u'TrumpGolf mdamelincourt', u'TrumpGolf TrumpPalmBeach', u'TrumpGolf Swary81', u'TrumpGolf HanksYanksGolf', u'TrumpGolf JeffreyMoser', u'TrumpGolf AMurrayGolf', u'TrumpGolf worthy1961', u'TrumpGolf EricTrump']\n"
     ]
    }
   ],
   "source": [
    "path = \"edges.csv\"\n",
    "file_rdd = load_file(path)\n",
    "print file_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vertex(file_rdd):\n",
    "    \"\"\" Create a Vertex DataFrame\n",
    "    Args: \n",
    "        path (string): path to the file\n",
    "    Returns:\n",
    "        DataFrame[id: string]\n",
    "    \"\"\"\n",
    "    def get_vertex(line):\n",
    "        return line.split(' ')\n",
    "    v_rdd = file_rdd.flatMap(get_vertex)\n",
    "    v_rdd = v_rdd.map(lambda x: (x, )).distinct()\n",
    "    v_df =v_rdd.toDF(['id']) \n",
    "    return v_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code should get the result:\n",
    " ```\n",
    "    +---------------+\n",
    "    |             id|\n",
    "    +---------------+\n",
    "    |   HeyItsLindaC|\n",
    "    | vanderkimberly|\n",
    "    |  TiffanyATrump|\n",
    "    |     D29Gillian|\n",
    "    | HanksYanksGolf|\n",
    "    | MissVanZutphen|\n",
    "    |      foxnation|\n",
    "    |             AP|\n",
    "    |  CLewandowski_|\n",
    "    |   MELANIATRUMP|\n",
    "    |         JoeNBC|\n",
    "    |TrumpGolfPhilly|\n",
    "    |    TrumpPanama|\n",
    "    |     adwnewyork|\n",
    "    | KateRuthBrewer|\n",
    "    | KatrinaCampins|\n",
    "    |  IvankaJewelry|\n",
    "    | chelseahandler|\n",
    "    |          TNGCJ|\n",
    "    |    IvankaTrump|\n",
    "    +---------------+\n",
    "    only showing top 20 rows\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|             id|\n",
      "+---------------+\n",
      "|   EricTrumpFdn|\n",
      "|   TrumpNewYork|\n",
      "|     TrumpTower|\n",
      "|      TrumpGolf|\n",
      "|          Trump|\n",
      "|  TiffanyATrump|\n",
      "|   TrumpWaikiki|\n",
      "| MichaelBreedGA|\n",
      "|    ericbolling|\n",
      "|    trumpwinery|\n",
      "| TrumpTurnberry|\n",
      "|realDonaldTrump|\n",
      "|   TrumpJupiter|\n",
      "|  Cmiddaughgolf|\n",
      "|       parscale|\n",
      "|   JeffreyMoser|\n",
      "|   MichaelBreed|\n",
      "|  TrumpScotland|\n",
      "|      foxnation|\n",
      "|    TrumpGolfHV|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_df = create_vertex(file_rdd)\n",
    "v_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_edge(file_rdd):\n",
    "    \"\"\" Create a Edge DataFrame\n",
    "    Args: \n",
    "        MapPartitionsRDD: input file\n",
    "    Returns:\n",
    "        DataFrame[src: string, dst: string, relationship: string]\n",
    "        sort by src\n",
    "    \"\"\"\n",
    "    e_rdd = file_rdd.map(lambda line: tuple((line.split(' ')[0],\\\n",
    "                                              line.split(' ')[1],'friend')))\n",
    "    e_df = e_rdd.toDF(['src','dst','relationship'])\n",
    "    e_df = e_df.sort('src')\n",
    "    return e_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code should get the result:\n",
    "```\n",
    "    +---------------+---------------+------------+\n",
    "    |            src|            dst|relationship|\n",
    "    +---------------+---------------+------------+\n",
    "    |realDonaldTrump|          Trump|      friend|\n",
    "    |realDonaldTrump|      TrumpGolf|      friend|\n",
    "    |realDonaldTrump|  TiffanyATrump|      friend|\n",
    "    |realDonaldTrump|  IngrahamAngle|      friend|\n",
    "    |realDonaldTrump|     mike_pence|      friend|\n",
    "    |realDonaldTrump|      TeamTrump|      friend|\n",
    "    |realDonaldTrump|  DRUDGE_REPORT|      friend|\n",
    "    |realDonaldTrump|MrsVanessaTrump|      friend|\n",
    "    |realDonaldTrump|   LaraLeaTrump|      friend|\n",
    "    |realDonaldTrump|    seanhannity|      friend|\n",
    "    |realDonaldTrump|      foxnation|      friend|\n",
    "    |realDonaldTrump|  CLewandowski_|      friend|\n",
    "    |realDonaldTrump|     AnnCoulter|      friend|\n",
    "    |realDonaldTrump| DiamondandSilk|      friend|\n",
    "    |realDonaldTrump| KatrinaCampins|      friend|\n",
    "    |realDonaldTrump| KatrinaPierson|      friend|\n",
    "    |realDonaldTrump|MichaelCohen212|      friend|\n",
    "    |realDonaldTrump|  foxandfriends|      friend|\n",
    "    |realDonaldTrump|   MELANIATRUMP|      friend|\n",
    "    |realDonaldTrump|  GeraldoRivera|      friend|\n",
    "    +---------------+---------------+------------+\n",
    "    only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------+\n",
      "|  src|            dst|relationship|\n",
      "+-----+---------------+------------+\n",
      "|Trump|  TiffanyATrump|      friend|\n",
      "|Trump|   TrumpJupiter|      friend|\n",
      "|Trump| TrumpPalmBeach|      friend|\n",
      "|Trump|      TrumpGolf|      friend|\n",
      "|Trump|     TrumpTower|      friend|\n",
      "|Trump|    trumpwinery|      friend|\n",
      "|Trump| TrumpTurnberry|      friend|\n",
      "|Trump|TrumpBedminster|      friend|\n",
      "|Trump|TrumpNationalNY|      friend|\n",
      "|Trump|  TrumpScotland|      friend|\n",
      "|Trump|   TrumpWaikiki|      friend|\n",
      "|Trump|   TrumpDoonbeg|      friend|\n",
      "|Trump|    TrumpPanama|      friend|\n",
      "|Trump|   TrumpToronto|      friend|\n",
      "|Trump|    TrumpHotels|      friend|\n",
      "|Trump|   TrumpChicago|      friend|\n",
      "|Trump|  TrumpLasVegas|      friend|\n",
      "|Trump|TrumpGolfPhilly|      friend|\n",
      "|Trump| TrumpColtsNeck|      friend|\n",
      "|Trump|    TrumpGolfHV|      friend|\n",
      "+-----+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df = create_edge(file_rdd)\n",
    "e_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pagerank(v_df, e_df, d=0.85, iters=100):\n",
    "    \"\"\" Create a DataFrame with 'id' and 'pagerank' column\n",
    "    Args: \n",
    "        MapPartitionsRDD: input file\n",
    "    Returns:\n",
    "        DataFrame[id: string, pagerank: double]: a DataFrame with 'id' and 'pagerank' column\n",
    "    \"\"\"\n",
    "    g = GraphFrame(v_df, e_df)\n",
    "    res = g.pageRank(resetProbability= 1 - d, maxIter=iters)\n",
    "    res = res.vertices.select(\"id\", \"pagerank\")\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code should get the result:\n",
    "```\n",
    "    +---------------+-------------------+\n",
    "    |             id|           pagerank|\n",
    "    +---------------+-------------------+\n",
    "    | KatrinaPierson| 0.1531138204693519|\n",
    "    |   MichaelBreed|0.15580118291790124|\n",
    "    |    TrumpHotels|0.15385936436797615|\n",
    "    |  TrumpScotland|0.15385936436797615|\n",
    "    |  foxandfriends| 0.1531138204693519|\n",
    "    |      foxnation| 0.1531138204693519|\n",
    "    |   CallawayGolf|0.15580118291790124|\n",
    "    |    TrumpGolfHV|0.15385936436797615|\n",
    "    |     TrumpDoral|0.15697318483732803|\n",
    "    |TrumpFerryPoint|0.15385936436797615|\n",
    "    |  oreillyfactor| 0.1531138204693519|\n",
    "    |  CLewandowski_| 0.1531138204693519|\n",
    "    |   VinceMcMahon| 0.1531138204693519|\n",
    "    |  DRUDGE_REPORT| 0.1531138204693519|\n",
    "    |     SIGolfPlus|0.15580118291790124|\n",
    "    |   MELANIATRUMP| 0.1531138204693519|\n",
    "    |     DanScavino| 0.1531138204693519|\n",
    "    |    CRTurnberry|0.15580118291790124|\n",
    "    |MichaelCohen212| 0.1531138204693519|\n",
    "    |         JoeNBC| 0.1531138204693519|\n",
    "    +---------------+-------------------+\n",
    "    only showing top 20 rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|            id|           pagerank|\n",
      "+--------------+-------------------+\n",
      "|   AMurrayGolf|0.15580118291790124|\n",
      "|    AnnCoulter| 0.1531138204693519|\n",
      "|BrendanAMurphy|0.15580118291790124|\n",
      "| CLewandowski_| 0.1531138204693519|\n",
      "|   CRTurnberry|0.15580118291790124|\n",
      "|  CallawayGolf|0.15580118291790124|\n",
      "| Cmiddaughgolf|0.15580118291790124|\n",
      "| DRUDGE_REPORT| 0.1531138204693519|\n",
      "|    DanScavino| 0.1531138204693519|\n",
      "|DiamondandSilk| 0.1531138204693519|\n",
      "|DonaldJTrumpJr|0.15697318483732803|\n",
      "|     EricTrump| 0.1627743677552293|\n",
      "|  EricTrumpFdn|0.15385936436797615|\n",
      "| GeraldoRivera| 0.1531138204693519|\n",
      "|HanksYanksGolf|0.15580118291790124|\n",
      "| IngrahamAngle| 0.1531138204693519|\n",
      "|   IvankaTrump|0.15697318483732803|\n",
      "|  JeffreyMoser|0.15580118291790124|\n",
      "|        JoeNBC| 0.1531138204693519|\n",
      "|KatrinaCampins| 0.1531138204693519|\n",
      "+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank = get_pagerank(v_df, e_df)\n",
    "pagerank.sort('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Q3: Implement your own PageRank Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we are getting familiar Spark, now, we are going to implement our own PageRank function using Spark. Since in Spark, most of our process needs to be done in lambda function, so, in the following function, we need to yield result to get generator.  \n",
    "First, we need to parse every line into tuple(followee, follower) and group them together.  \n",
    "You may use lambda function or pre-defined function parse_relations to parse input line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to implement some function that will be passed to Spark\n",
    "- parse_relations: parse input line into a pair (follower, followee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_relations(line):\n",
    "        \"\"\"This function will pass to map function to split a line into follower and followee\n",
    "        Args: \n",
    "            line(String): input line\n",
    "        Returns:\n",
    "            tuple: tuple of follower and followee\n",
    "        \"\"\"\n",
    "        words = line.split(' ')\n",
    "        return (words[0], words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- init_rank: in this function, we need to init the input pair and give each follower an init rank 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_rank(edge):\n",
    "    \"\"\"Initial the start rank of each node\n",
    "    Args: \n",
    "        tuple: (node, <pyspark.resultiterable.ResultIterable object>)\n",
    "    Returns:\n",
    "        tuple: tuples of (follower, 1.0)\n",
    "    \"\"\"\n",
    "    follower = edge[0]\n",
    "    return follower, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute_contributes : in this function, you will need to write a generator to calculate node's contribution to the rank of other URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeContribs(nodes, rank):\n",
    "    num_nodes = len(nodes)\n",
    "    for node in nodes:\n",
    "        yield (node, rank / num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate new rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_rank(rank):\n",
    "    \"\"\"Calculate new rank\n",
    "    Args: \n",
    "        rank(int): previous rank\n",
    "    Returns:\n",
    "        int: new rank\n",
    "    \"\"\"\n",
    "    new_rank = rank * 0.85 + 0.15\n",
    "    return new_rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_contribution(rank):\n",
    "    return computeContribs(rank[1][0], rank[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect all together: Using the functions implemented before finish page_rank function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def page_rank(file_rdd):\n",
    "    \"\"\"Load file into RDD\n",
    "    Args: \n",
    "        MapPartitionsRDD: input file\n",
    "    Returns:\n",
    "        list: a list of tuples of node and PageRank, sort by key\n",
    "    \"\"\"\n",
    "        \n",
    "    relations = file_rdd.map(parse_relations)\n",
    "    edges = relations.distinct().groupByKey()\n",
    "    ranks = edges.map(init_rank)\n",
    "    for iteration in range(10):\n",
    "        # Calculates current node contributions to the rank of other nodes.\n",
    "        contribs = edges.join(ranks).flatMap(update_contribution)\n",
    "        # Re-calculates current node's ranks based on neighbor nodes.\n",
    "        ranks = contribs.reduceByKey(add)\n",
    "        ranks = ranks.mapValues(new_rank)\n",
    "    ranks = ranks.sortByKey()\n",
    "    res = ranks.collect()\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finished the page_rank function, run the following code to calculate pagerank of the input file\n",
    "You will get result:\n",
    "\n",
    "```\n",
    "    [(u'TrumpChicago', 0.15697318483734526),\n",
    "     (u'GeraldoRivera', 0.15311382046935743),\n",
    "     (u'piersmorgan', 0.15311382046935743),\n",
    "     (u'TrumpBedminster', 0.15385936436798783),\n",
    "     (u'CLewandowski_', 0.15311382046935743),\n",
    "     (u'parscale', 0.1558011829179162),\n",
    "     (u'IvankaTrump', 0.15697318483734526),\n",
    "     (u'TrumpPalmBeach', 0.15966054728590404),\n",
    "     (u'greta', 0.15311382046935743),\n",
    "     (u'foxandfriends', 0.15311382046935743),\n",
    "     (u'SIGolfPlus', 0.1558011829179162),\n",
    "     ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'AMurrayGolf', 0.1558011829179162),\n",
       " (u'AnnCoulter', 0.15311382046935743),\n",
       " (u'BrendanAMurphy', 0.1558011829179162),\n",
       " (u'CLewandowski_', 0.15311382046935743),\n",
       " (u'CRTurnberry', 0.1558011829179162),\n",
       " (u'CallawayGolf', 0.1558011829179162),\n",
       " (u'Cmiddaughgolf', 0.1558011829179162),\n",
       " (u'DRUDGE_REPORT', 0.15311382046935743),\n",
       " (u'DanScavino', 0.15311382046935743),\n",
       " (u'DiamondandSilk', 0.15311382046935743),\n",
       " (u'DonaldJTrumpJr', 0.15697318483734526),\n",
       " (u'EricTrump', 0.16277436775526147),\n",
       " (u'EricTrumpFdn', 0.15385936436798783),\n",
       " (u'GeraldoRivera', 0.15311382046935743),\n",
       " (u'HanksYanksGolf', 0.1558011829179162),\n",
       " (u'IngrahamAngle', 0.15311382046935743),\n",
       " (u'IvankaTrump', 0.15697318483734526),\n",
       " (u'JeffreyMoser', 0.1558011829179162),\n",
       " (u'JoeNBC', 0.15311382046935743),\n",
       " (u'KatrinaCampins', 0.15311382046935743),\n",
       " (u'KatrinaPierson', 0.15311382046935743),\n",
       " (u'LaraLeaTrump', 0.15311382046935743),\n",
       " (u'MELANIATRUMP', 0.15311382046935743),\n",
       " (u'MagicJohnson', 0.15311382046935743),\n",
       " (u'MarkBurnettTV', 0.15311382046935743),\n",
       " (u'MichaelBreed', 0.1558011829179162),\n",
       " (u'MichaelBreedGA', 0.1558011829179162),\n",
       " (u'MichaelCohen212', 0.15311382046935743),\n",
       " (u'MrsVanessaTrump', 0.15311382046935743),\n",
       " (u'PGA_JohnDaly', 0.1558011829179162),\n",
       " (u'RealRomaDowney', 0.15311382046935743),\n",
       " (u'SIGolfPlus', 0.1558011829179162),\n",
       " (u'SMURFpga', 0.1558011829179162),\n",
       " (u'Swary81', 0.1558011829179162),\n",
       " (u'TeamTrump', 0.15311382046935743),\n",
       " (u'TiffanyATrump', 0.15697318483734526),\n",
       " (u'Trump', 0.15891500338727363),\n",
       " (u'TrumpBedminster', 0.15385936436798783),\n",
       " (u'TrumpCharlotte', 0.15697318483734526),\n",
       " (u'TrumpChicago', 0.15697318483734526),\n",
       " (u'TrumpColtsNeck', 0.15385936436798783),\n",
       " (u'TrumpDC', 0.15385936436798783),\n",
       " (u'TrumpDoonbeg', 0.15385936436798783),\n",
       " (u'TrumpDoral', 0.15697318483734526),\n",
       " (u'TrumpFerryPoint', 0.15385936436798783),\n",
       " (u'TrumpGolf', 0.15697318483734526),\n",
       " (u'TrumpGolfDC', 0.15697318483734526),\n",
       " (u'TrumpGolfHV', 0.15385936436798783),\n",
       " (u'TrumpGolfLA', 0.15697318483734526),\n",
       " (u'TrumpGolfPhilly', 0.15385936436798783),\n",
       " (u'TrumpHotels', 0.15385936436798783),\n",
       " (u'TrumpJupiter', 0.15966054728590404),\n",
       " (u'TrumpLasVegas', 0.15697318483734526),\n",
       " (u'TrumpModels', 0.15385936436798783),\n",
       " (u'TrumpNationalNY', 0.15385936436798783),\n",
       " (u'TrumpNewYork', 0.15385936436798783),\n",
       " (u'TrumpPalmBeach', 0.15966054728590404),\n",
       " (u'TrumpPanama', 0.15385936436798783),\n",
       " (u'TrumpRealty', 0.15385936436798783),\n",
       " (u'TrumpScotland', 0.15385936436798783),\n",
       " (u'TrumpSoHo', 0.15385936436798783),\n",
       " (u'TrumpToronto', 0.15385936436798783),\n",
       " (u'TrumpTower', 0.15385936436798783),\n",
       " (u'TrumpTurnberry', 0.15385936436798783),\n",
       " (u'TrumpWaikiki', 0.15697318483734526),\n",
       " (u'VinceMcMahon', 0.15311382046935743),\n",
       " (u'ericbolling', 0.15311382046935743),\n",
       " (u'foxandfriends', 0.15311382046935743),\n",
       " (u'foxnation', 0.15311382046935743),\n",
       " (u'garyplayer', 0.15311382046935743),\n",
       " (u'greta', 0.15311382046935743),\n",
       " (u'horsebalmedie', 0.1558011829179162),\n",
       " (u'joshspragins', 0.1558011829179162),\n",
       " (u'livelasvegas', 0.1558011829179162),\n",
       " (u'mdamelincourt', 0.1558011829179162),\n",
       " (u'mike_pence', 0.15311382046935743),\n",
       " (u'morningmika', 0.15311382046935743),\n",
       " (u'oreillyfactor', 0.15311382046935743),\n",
       " (u'parscale', 0.1558011829179162),\n",
       " (u'piersmorgan', 0.15311382046935743),\n",
       " (u'realDonaldTrump', 0.15385936436798783),\n",
       " (u'seanhannity', 0.15311382046935743),\n",
       " (u'trumpwinery', 0.15385936436798783),\n",
       " (u'worthy1961', 0.1558011829179162)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_rank(file_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some thoughts beyond this tutorial:\n",
    "If you are trying to change the iteration times to a larger number, there will be a highly probablity that the code will crash. To figure this out, we need to get a deeper understanding of RDD.\n",
    "RDD has two parts five components:\n",
    "- lineage info: Partitions, dependencies, computation(like map,filter,join).\n",
    "- optimization info: partitioner, preferred locations.\n",
    "\n",
    "In Spark and other distributed system, Fault Recovery is very important, for MPI, because it is not convenient to achieve Fault Recovery, we need to save checkpoint to make MPI structure can be Fault Recovery. So if we start iteration, the Fault Recovery will become larger and larger.\n",
    "\n",
    "We can assume that if the probability of a task go wrong is p, and we have N tasks to do in one iteration. So the probability of this iteration does not need Fault Recovery is \n",
    "            $$p^N$$\n",
    "\n",
    "So from iteration 0 to iteration K, the prob is:\n",
    "            $$p^{N*(k-1)}*(1 - p^N)$$\n",
    "\n",
    "From above porb, we can find that with the iteration number increase, we need more and more resource for Fault Recovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
